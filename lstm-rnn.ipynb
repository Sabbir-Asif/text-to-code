{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets tokenizers nltk sacrebleu -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:14.290437Z","iopub.execute_input":"2026-02-15T05:32:14.291181Z","iopub.status.idle":"2026-02-15T05:32:18.688427Z","shell.execute_reply.started":"2026-02-15T05:32:14.291150Z","shell.execute_reply":"2026-02-15T05:32:18.687566Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport numpy as np\nimport random\nimport os\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:18.690190Z","iopub.execute_input":"2026-02-15T05:32:18.690545Z","iopub.status.idle":"2026-02-15T05:32:25.624794Z","shell.execute_reply.started":"2026-02-15T05:32:18.690518Z","shell.execute_reply":"2026-02-15T05:32:25.624010Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Configuration\nMAX_DOC_LEN = 40\nMAX_CODE_LEN = 80\nBATCH_SIZE = 32\nEMB_DIM = 256\nHID_DIM = 512\nNUM_LAYERS = 2\nDROPOUT = 0.3\nEPOCHS = 10\nLR = 1e-3\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:25.625646Z","iopub.execute_input":"2026-02-15T05:32:25.626038Z","iopub.status.idle":"2026-02-15T05:32:25.684161Z","shell.execute_reply.started":"2026-02-15T05:32:25.626014Z","shell.execute_reply":"2026-02-15T05:32:25.683319Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load dataset\ndataset = load_dataset(\"Nan-Do/code-search-net-python\")\nfull_data = dataset[\"train\"].select(range(7000))\n\nsplit1 = full_data.train_test_split(test_size=0.2, seed=42)\ntrain_data = split1[\"train\"]\ntemp_data = split1[\"test\"]\n\nsplit2 = temp_data.train_test_split(test_size=0.5, seed=42)\nval_data = split2[\"train\"]\ntest_data = split2[\"test\"]\n\nprint(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:25.686180Z","iopub.execute_input":"2026-02-15T05:32:25.686670Z","iopub.status.idle":"2026-02-15T05:32:35.674483Z","shell.execute_reply.started":"2026-02-15T05:32:25.686648Z","shell.execute_reply":"2026-02-15T05:32:35.673900Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df72e220a7874eb3814b6b47f405e8a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00004-ee77a7de79eb2a(…):   0%|          | 0.00/155M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e73a54c34d4243dabd64aa10cce1474a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00004-648b3bede2edf6(…):   0%|          | 0.00/139M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6da903320a34415ab61e470accc7210"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00002-of-00004-1dfd72b171e6b2(…):   0%|          | 0.00/153M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa373a8fbc77423c882cc6dde247b3b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00003-of-00004-184ab6d0e3c690(…):   0%|          | 0.00/151M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df93363cdcd64105bdfe1e370847e2c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/455243 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8a4b3e5d78c41179428ddf3d03c1f0b"}},"metadata":{}},{"name":"stdout","text":"Train: 5600, Val: 700, Test: 700\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Tokenizer setup\ndef setup_tokenizer(train_data, vocab_size=8000):\n    tokenizer_path = \"tokenizer.json\"\n    \n    if os.path.exists(tokenizer_path):\n        tokenizer = Tokenizer.from_file(tokenizer_path)\n        print(\"Loaded existing tokenizer\")\n    else:\n        print(\"Training new tokenizer...\")\n        tokenizer = Tokenizer(models.BPE(unk_token=\"<UNK>\"))\n        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n        \n        trainer = trainers.BpeTrainer(\n            vocab_size=vocab_size,\n            special_tokens=[\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]\n        )\n        \n        corpus = []\n        for i in range(min(5000, len(train_data))):\n            corpus.append(train_data[i][\"docstring\"])\n            corpus.append(train_data[i][\"code\"])\n        \n        tokenizer.train_from_iterator(corpus, trainer)\n        tokenizer.save(tokenizer_path)\n        print(\"Tokenizer trained and saved\")\n    \n    # Enable padding and truncation\n    tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"<PAD>\"), \n                            pad_token=\"<PAD>\", \n                            length=MAX_CODE_LEN)\n    tokenizer.enable_truncation(max_length=MAX_CODE_LEN)\n    \n    return tokenizer\n\ntokenizer = setup_tokenizer(train_data)\n\n# Token helpers\nPAD_IDX = tokenizer.token_to_id(\"<PAD>\")\nSOS_IDX = tokenizer.token_to_id(\"<SOS>\")\nEOS_IDX = tokenizer.token_to_id(\"<EOS>\")\nUNK_IDX = tokenizer.token_to_id(\"<UNK>\")\nVOCAB_SIZE = tokenizer.get_vocab_size()\n\nprint(f\"Vocab size: {VOCAB_SIZE}\")\nprint(f\"PAD: {PAD_IDX}, SOS: {SOS_IDX}, EOS: {EOS_IDX}, UNK: {UNK_IDX}\")\n\ndef encode(text):\n    return tokenizer.encode(text).ids\n\ndef decode(ids):\n    return tokenizer.decode(ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:35.675268Z","iopub.execute_input":"2026-02-15T05:32:35.675538Z","iopub.status.idle":"2026-02-15T05:32:39.331405Z","shell.execute_reply.started":"2026-02-15T05:32:35.675510Z","shell.execute_reply":"2026-02-15T05:32:39.330629Z"}},"outputs":[{"name":"stdout","text":"Training new tokenizer...\n\n\n\nTokenizer trained and saved\nVocab size: 8000\nPAD: 0, SOS: 1, EOS: 2, UNK: 3\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Dataset\nclass CodeDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Ensure we have valid strings\n        docstring = item[\"docstring\"] if item[\"docstring\"] else \"\"\n        code = item[\"code\"] if item[\"code\"] else \"\"\n        \n        doc = encode(docstring)[:MAX_DOC_LEN]\n        code = encode(code)[:MAX_CODE_LEN]\n        \n        # Ensure sequences are not empty\n        if len(doc) == 0:\n            doc = [PAD_IDX]\n        if len(code) == 0:\n            code = [PAD_IDX]\n            \n        return {\"doc\": doc, \"code\": code}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:39.332382Z","iopub.execute_input":"2026-02-15T05:32:39.332666Z","iopub.status.idle":"2026-02-15T05:32:39.338118Z","shell.execute_reply.started":"2026-02-15T05:32:39.332643Z","shell.execute_reply":"2026-02-15T05:32:39.337309Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Collate function\ndef collate_fn(batch):\n    docs = [b[\"doc\"] for b in batch]\n    codes = [b[\"code\"] for b in batch]\n    \n    max_doc = max(len(d) for d in docs)\n    max_code = max(len(c) for c in codes)\n    \n    src_list, trg_in_list, trg_out_list = [], [], []\n    \n    for d, c in zip(docs, codes):\n        # Pad source\n        src_list.append(d + [PAD_IDX] * (max_doc - len(d)))\n        \n        # Target input (with SOS)\n        trg_in_list.append([SOS_IDX] + c + [PAD_IDX] * (max_code - len(c)))\n        \n        # Target output (with EOS)\n        trg_out_list.append(c + [EOS_IDX] + [PAD_IDX] * (max_code - len(c)))\n    \n    return {\n        \"src\": torch.tensor(src_list, dtype=torch.long),\n        \"trg_in\": torch.tensor(trg_in_list, dtype=torch.long),\n        \"trg_out\": torch.tensor(trg_out_list, dtype=torch.long)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:39.338972Z","iopub.execute_input":"2026-02-15T05:32:39.339220Z","iopub.status.idle":"2026-02-15T05:32:39.353337Z","shell.execute_reply.started":"2026-02-15T05:32:39.339198Z","shell.execute_reply":"2026-02-15T05:32:39.352618Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# DataLoaders\ntrain_loader = DataLoader(CodeDataset(train_data), batch_size=BATCH_SIZE, \n                         shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(CodeDataset(val_data), batch_size=BATCH_SIZE, \n                       collate_fn=collate_fn)\ntest_loader = DataLoader(CodeDataset(test_data), batch_size=BATCH_SIZE, \n                        collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:39.354239Z","iopub.execute_input":"2026-02-15T05:32:39.354532Z","iopub.status.idle":"2026-02-15T05:32:39.364494Z","shell.execute_reply.started":"2026-02-15T05:32:39.354503Z","shell.execute_reply":"2026-02-15T05:32:39.363884Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Encoder\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, num_layers=2, dropout=0.3):\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n        self.dropout = nn.Dropout(dropout)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n                          dropout=dropout, batch_first=False)\n        \n    def forward(self, src):\n        # src shape: (seq_len, batch_size)\n        embedded = self.dropout(self.embedding(src))\n        \n        # Initialize hidden states\n        batch_size = src.shape[1]\n        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(src.device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(src.device)\n        \n        outputs, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        return hidden, cell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:39.365348Z","iopub.execute_input":"2026-02-15T05:32:39.365590Z","iopub.status.idle":"2026-02-15T05:32:39.375520Z","shell.execute_reply.started":"2026-02-15T05:32:39.365542Z","shell.execute_reply":"2026-02-15T05:32:39.374882Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Decoder\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, num_layers=2, dropout=0.3):\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n        self.dropout = nn.Dropout(dropout)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n                          dropout=dropout, batch_first=False)\n        self.fc = nn.Linear(hid_dim, output_dim)\n        \n    def forward(self, x, hidden, cell):\n        # x shape: (batch_size)\n        x = x.unsqueeze(0)  # (1, batch_size)\n        embedded = self.dropout(self.embedding(x))\n        \n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        prediction = self.fc(output.squeeze(0))  # (batch_size, output_dim)\n        \n        return prediction, hidden, cell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:39.377637Z","iopub.execute_input":"2026-02-15T05:32:39.377858Z","iopub.status.idle":"2026-02-15T05:32:39.389986Z","shell.execute_reply.started":"2026-02-15T05:32:39.377839Z","shell.execute_reply":"2026-02-15T05:32:39.389456Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        # src: (seq_len, batch_size)\n        # trg: (seq_len, batch_size)\n        \n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        vocab_size = self.decoder.fc.out_features\n        \n        outputs = torch.zeros(trg_len, batch_size, vocab_size).to(self.device)\n        \n        hidden, cell = self.encoder(src)\n        x = trg[0]  # First input is SOS token\n        \n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            \n            # Teacher forcing\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            x = trg[t] if teacher_force else top1\n            \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:39.390702Z","iopub.execute_input":"2026-02-15T05:32:39.390907Z","iopub.status.idle":"2026-02-15T05:32:39.401607Z","shell.execute_reply.started":"2026-02-15T05:32:39.390886Z","shell.execute_reply":"2026-02-15T05:32:39.401034Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Initialize model\nencoder = Encoder(VOCAB_SIZE, EMB_DIM, HID_DIM, NUM_LAYERS, DROPOUT)\ndecoder = Decoder(VOCAB_SIZE, EMB_DIM, HID_DIM, NUM_LAYERS, DROPOUT)\nmodel = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n\n# Count parameters\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"The model has {count_parameters(model):,} trainable parameters\")\n\n# Optimizer and loss\noptimizer = optim.Adam(model.parameters(), lr=LR)\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:39.402446Z","iopub.execute_input":"2026-02-15T05:32:39.402712Z","iopub.status.idle":"2026-02-15T05:32:42.931093Z","shell.execute_reply.started":"2026-02-15T05:32:39.402691Z","shell.execute_reply":"2026-02-15T05:32:42.930146Z"}},"outputs":[{"name":"stdout","text":"The model has 15,556,416 trainable parameters\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Training function\ndef train_epoch(model, loader, optimizer, criterion, clip=1):\n    model.train()\n    epoch_loss = 0\n    \n    for batch in tqdm(loader, desc=\"Training\"):\n        src = batch[\"src\"].transpose(0, 1).to(DEVICE)\n        trg_in = batch[\"trg_in\"].transpose(0, 1).to(DEVICE)\n        trg_out = batch[\"trg_out\"].transpose(0, 1).to(DEVICE)\n        \n        optimizer.zero_grad()\n        \n        output = model(src, trg_in)\n        \n        # Reshape for loss\n        output = output[1:].reshape(-1, output.shape[-1])\n        trg_out = trg_out[1:].reshape(-1)\n        \n        loss = criterion(output, trg_out)\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n    \n    return epoch_loss / len(loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:42.931998Z","iopub.execute_input":"2026-02-15T05:32:42.932619Z","iopub.status.idle":"2026-02-15T05:32:42.939274Z","shell.execute_reply.started":"2026-02-15T05:32:42.932583Z","shell.execute_reply":"2026-02-15T05:32:42.938629Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Evaluation function\ndef evaluate_epoch(model, loader, criterion):\n    model.eval()\n    epoch_loss = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Evaluating\"):\n            src = batch[\"src\"].transpose(0, 1).to(DEVICE)\n            trg_in = batch[\"trg_in\"].transpose(0, 1).to(DEVICE)\n            trg_out = batch[\"trg_out\"].transpose(0, 1).to(DEVICE)\n            \n            output = model(src, trg_in, teacher_forcing_ratio=0)\n            \n            output = output[1:].reshape(-1, output.shape[-1])\n            trg_out = trg_out[1:].reshape(-1)\n            \n            loss = criterion(output, trg_out)\n            epoch_loss += loss.item()\n    \n    return epoch_loss / len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:42.940071Z","iopub.execute_input":"2026-02-15T05:32:42.940348Z","iopub.status.idle":"2026-02-15T05:32:52.221643Z","shell.execute_reply.started":"2026-02-15T05:32:42.940308Z","shell.execute_reply":"2026-02-15T05:32:52.220812Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Training loop\nbest_val_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n    val_loss = evaluate_epoch(model, val_loader, criterion)\n    \n    print(f\"Epoch {epoch+1:02}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), \"best_model_lstm.pt\")\n        print(f\"✓ Saved best model with val loss: {val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:32:52.222610Z","iopub.execute_input":"2026-02-15T05:32:52.222869Z","iopub.status.idle":"2026-02-15T05:39:58.984597Z","shell.execute_reply.started":"2026-02-15T05:32:52.222847Z","shell.execute_reply":"2026-02-15T05:39:58.983847Z"}},"outputs":[{"name":"stderr","text":"Training: 100%|██████████| 175/175 [00:40<00:00,  4.32it/s]\nEvaluating: 100%|██████████| 22/22 [00:02<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01: Train Loss = 6.6759, Val Loss = 6.6036\n✓ Saved best model with val loss: 6.6036\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 175/175 [00:39<00:00,  4.40it/s]\nEvaluating: 100%|██████████| 22/22 [00:02<00:00,  8.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02: Train Loss = 6.3142, Val Loss = 6.6697\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 175/175 [00:39<00:00,  4.39it/s]\nEvaluating: 100%|██████████| 22/22 [00:02<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03: Train Loss = 6.1330, Val Loss = 6.6012\n✓ Saved best model with val loss: 6.6012\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 175/175 [00:39<00:00,  4.39it/s]\nEvaluating: 100%|██████████| 22/22 [00:02<00:00,  8.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04: Train Loss = 6.0043, Val Loss = 6.6833\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 175/175 [00:40<00:00,  4.31it/s]\nEvaluating: 100%|██████████| 22/22 [00:02<00:00,  8.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05: Train Loss = 5.8931, Val Loss = 6.4968\n✓ Saved best model with val loss: 6.4968\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 175/175 [00:40<00:00,  4.37it/s]\nEvaluating: 100%|██████████| 22/22 [00:02<00:00,  8.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06: Train Loss = 5.7868, Val Loss = 6.4615\n✓ Saved best model with val loss: 6.4615\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 175/175 [00:39<00:00,  4.39it/s]\nEvaluating: 100%|██████████| 22/22 [00:02<00:00,  8.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07: Train Loss = 5.6818, Val Loss = 6.4578\n✓ Saved best model with val loss: 6.4578\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 175/175 [00:39<00:00,  4.38it/s]\nEvaluating: 100%|██████████| 22/22 [00:02<00:00,  8.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08: Train Loss = 5.5931, Val Loss = 6.4002\n✓ Saved best model with val loss: 6.4002\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 175/175 [00:39<00:00,  4.38it/s]\nEvaluating: 100%|██████████| 22/22 [00:02<00:00,  8.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09: Train Loss = 5.4952, Val Loss = 6.5477\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 175/175 [00:39<00:00,  4.38it/s]\nEvaluating: 100%|██████████| 22/22 [00:02<00:00,  8.53it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss = 5.4142, Val Loss = 6.4204\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def generate_code(model, docstring, max_len=MAX_CODE_LEN):\n    model.eval()\n    \n    # Encode docstring\n    tokens = encode(docstring)[:MAX_DOC_LEN]\n    if len(tokens) == 0:\n        tokens = [PAD_IDX]\n    \n    src = torch.tensor(tokens).unsqueeze(1).to(DEVICE)\n    \n    with torch.no_grad():\n        hidden, cell = model.encoder(src)\n        \n        x = torch.tensor([SOS_IDX]).to(DEVICE)\n        outputs = []\n        \n        for _ in range(max_len):\n            output, hidden, cell = model.decoder(x, hidden, cell)\n            \n            # Use sampling for diversity\n            probs = torch.softmax(output, dim=1)\n            top1 = output.argmax(1).item()\n            \n            if top1 == EOS_IDX:\n                break\n                \n            outputs.append(top1)\n            x = torch.tensor([top1]).to(DEVICE)\n    \n    return decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:39:58.985686Z","iopub.execute_input":"2026-02-15T05:39:58.986037Z","iopub.status.idle":"2026-02-15T05:39:58.991812Z","shell.execute_reply.started":"2026-02-15T05:39:58.986014Z","shell.execute_reply":"2026-02-15T05:39:58.991197Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# BLEU score calculation\ndef calculate_bleu(model, test_data, n_samples=100):\n    smooth = SmoothingFunction().method1\n    scores = []\n    \n    for i in range(min(n_samples, len(test_data))):\n        ref = test_data[i][\"code\"]\n        doc = test_data[i][\"docstring\"]\n        \n        pred = generate_code(model, doc)\n        \n        # Use subword tokens for BLEU\n        ref_tokens = tokenizer.encode(ref).tokens\n        pred_tokens = tokenizer.encode(pred).tokens\n        \n        if len(ref_tokens) > 0 and len(pred_tokens) > 0:\n            try:\n                score = sentence_bleu([ref_tokens], pred_tokens, \n                                    smoothing_function=smooth)\n                scores.append(score)\n            except:\n                continue\n    \n    return np.mean(scores) if scores else 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:39:58.992820Z","iopub.execute_input":"2026-02-15T05:39:58.993073Z","iopub.status.idle":"2026-02-15T05:39:59.003997Z","shell.execute_reply.started":"2026-02-15T05:39:58.993053Z","shell.execute_reply":"2026-02-15T05:39:59.003355Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Final evaluation\nmodel.load_state_dict(torch.load(\"best_model_lstm.pt\"))\nmodel.to(DEVICE)\n\nbleu = calculate_bleu(model, test_data, n_samples=100)\nprint(f\"\\nBLEU Score on 50 test samples: {bleu:.4f}\")\n\n# Example generation\nprint(\"\\n\" + \"=\"*50)\nprint(\"Example Generation:\")\nprint(\"-\"*50)\nidx = random.randint(0, len(test_data)-1)\ndocstring = test_data[idx][\"docstring\"]\nreference = test_data[idx][\"code\"]\ngenerated = generate_code(model, docstring)\n\nprint(f\"Docstring: {docstring[:100]}...\" if len(docstring) > 100 else f\"Docstring: {docstring}\")\nprint(f\"\\nGenerated:\\n{generated[:200]}...\" if len(generated) > 200 else f\"\\nGenerated:\\n{generated}\")\nprint(f\"\\nReference:\\n{reference[:200]}...\" if len(reference) > 200 else f\"\\nReference:\\n{reference}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T05:42:46.602410Z","iopub.execute_input":"2026-02-15T05:42:46.603170Z","iopub.status.idle":"2026-02-15T05:42:50.548152Z","shell.execute_reply.started":"2026-02-15T05:42:46.603140Z","shell.execute_reply":"2026-02-15T05:42:50.547538Z"}},"outputs":[{"name":"stdout","text":"\nBLEU Score on 50 test samples: 0.0067\n\n==================================================\nExample Generation:\n--------------------------------------------------\nDocstring: Decorator for Layers, overriding add_weight for trainable initializers.\n\nGenerated:\n_ ( ( ( , \"\"\" \"\"\" the the the the the .\"\"\" = . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . =\n\nReference:\ndef add_weight(cls):\n  \"\"\"Decorator for Layers, overriding add_weight for trainable initializers.\"\"\"\n  @functools.wraps(cls.add_weight)\n  def _add_weight(self,\n                  name=None,\n           ...\n","output_type":"stream"}],"execution_count":23}]}