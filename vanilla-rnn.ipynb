{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets transformers nltk sentencepiece","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:16:06.370831Z","iopub.execute_input":"2026-02-14T06:16:06.371469Z","iopub.status.idle":"2026-02-14T06:16:09.774368Z","shell.execute_reply.started":"2026-02-14T06:16:06.371441Z","shell.execute_reply":"2026-02-14T06:16:09.773590Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.2)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nRequirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.2)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.6.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom nltk.translate.bleu_score import sentence_bleu\nimport random\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:16:09.776419Z","iopub.execute_input":"2026-02-14T06:16:09.776789Z","iopub.status.idle":"2026-02-14T06:16:09.781343Z","shell.execute_reply.started":"2026-02-14T06:16:09.776759Z","shell.execute_reply":"2026-02-14T06:16:09.780643Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"special_tokens = {\"pad_token\":\"<PAD>\", \"bos_token\":\"<SOS>\", \"eos_token\":\"<EOS>\"}\ntokenizer.add_special_tokens(special_tokens)\n\nPAD_IDX = tokenizer.pad_token_id\nSOS_IDX = tokenizer.bos_token_id\nEOS_IDX = tokenizer.eos_token_id\n\n# Now, get updated vocab size\nINPUT_DIM = len(tokenizer)\nOUTPUT_DIM = len(tokenizer)\n\nprint(\"Vocab size:\", INPUT_DIM)\nprint(\"PAD_IDX:\", PAD_IDX, \"SOS_IDX:\", SOS_IDX, \"EOS_IDX:\", EOS_IDX)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:16:09.782427Z","iopub.execute_input":"2026-02-14T06:16:09.782999Z","iopub.status.idle":"2026-02-14T06:16:09.811740Z","shell.execute_reply.started":"2026-02-14T06:16:09.782965Z","shell.execute_reply":"2026-02-14T06:16:09.810974Z"}},"outputs":[{"name":"stdout","text":"Vocab size: 30525\nPAD_IDX: 30522 SOS_IDX: 30523 EOS_IDX: 30524\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"dataset = load_dataset(\"Nan-Do/code-search-net-python\")\n\n# Take subset for faster training\nfull_data = dataset[\"train\"].select(range(7000))\n\n# Split 80% train, 10% val, 10% test\nsplit1 = full_data.train_test_split(test_size=0.2, seed=42)\ntrain_data = split1[\"train\"]\ntemp_data  = split1[\"test\"]\nsplit2 = temp_data.train_test_split(test_size=0.5, seed=42)\nval_data  = split2[\"train\"]\ntest_data = split2[\"test\"]\n\nprint(len(train_data), len(val_data), len(test_data))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:16:09.812715Z","iopub.execute_input":"2026-02-14T06:16:09.813096Z","iopub.status.idle":"2026-02-14T06:16:11.523120Z","shell.execute_reply.started":"2026-02-14T06:16:09.813060Z","shell.execute_reply":"2026-02-14T06:16:11.522318Z"}},"outputs":[{"name":"stdout","text":"5600 700 700\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"class CodeDataset(Dataset):\n    def __init__(self, data, tokenizer, max_doc_len=50, max_code_len=80):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_doc_len = max_doc_len\n        self.max_code_len = max_code_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        # Tokenize docstring\n        doc = self.tokenizer.encode(\n            item[\"docstring\"], truncation=True, max_length=self.max_doc_len, add_special_tokens=False\n        )\n        # Tokenize code\n        code = self.tokenizer.encode(\n            item[\"code\"], truncation=True, max_length=self.max_code_len, add_special_tokens=False\n        )\n        return {\"doc\": doc, \"code\": code}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:16:11.525108Z","iopub.execute_input":"2026-02-14T06:16:11.525654Z","iopub.status.idle":"2026-02-14T06:16:11.530469Z","shell.execute_reply.started":"2026-02-14T06:16:11.525616Z","shell.execute_reply":"2026-02-14T06:16:11.529946Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def collate_fn(batch):\n    docs = [b[\"doc\"] for b in batch]\n    codes = [b[\"code\"] for b in batch]\n    max_doc = max(len(d) for d in docs)\n    max_code = max(len(c) for c in codes)\n\n    doc_pad, trg_in, trg_out = [], [], []\n\n    for d,c in zip(docs,codes):\n        doc_pad.append(d + [PAD_IDX]*(max_doc-len(d)))\n        trg_in.append([SOS_IDX] + c + [PAD_IDX]*(max_code-len(c)))\n        trg_out.append(c + [EOS_IDX] + [PAD_IDX]*(max_code-len(c)))\n\n    return {\n        \"src\": torch.tensor(doc_pad),\n        \"trg_in\": torch.tensor(trg_in),\n        \"trg_out\": torch.tensor(trg_out)\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:16:11.531411Z","iopub.execute_input":"2026-02-14T06:16:11.531769Z","iopub.status.idle":"2026-02-14T06:16:11.541308Z","shell.execute_reply.started":"2026-02-14T06:16:11.531737Z","shell.execute_reply":"2026-02-14T06:16:11.540639Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"train_ds = CodeDataset(train_data, tokenizer)\nval_ds   = CodeDataset(val_data, tokenizer)\ntest_ds  = CodeDataset(test_data, tokenizer)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader   = DataLoader(val_ds, batch_size=32, collate_fn=collate_fn)\ntest_loader  = DataLoader(test_ds, batch_size=32, collate_fn=collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:16:11.542242Z","iopub.execute_input":"2026-02-14T06:16:11.542641Z","iopub.status.idle":"2026-02-14T06:16:11.555798Z","shell.execute_reply.started":"2026-02-14T06:16:11.542601Z","shell.execute_reply":"2026-02-14T06:16:11.554987Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n        self.rnn = nn.RNN(emb_dim, hid_dim)\n\n    def forward(self, src):\n        embedded = self.embedding(src)\n        _, hidden = self.rnn(embedded)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n        self.rnn = nn.RNN(emb_dim, hid_dim)\n        self.fc = nn.Linear(hid_dim, output_dim)\n\n    def forward(self, x, hidden):\n        x = x.unsqueeze(0)\n        emb = self.embedding(x)\n        out, hidden = self.rnn(emb, hidden)\n        pred = self.fc(out.squeeze(0))\n        return pred, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, enc, dec, device):\n        super().__init__()\n        self.enc = enc\n        self.dec = dec\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing=0.5):\n        batch = trg.shape[1]\n        trg_len = trg.shape[0]\n        vocab = self.dec.fc.out_features\n\n        outputs = torch.zeros(trg_len, batch, vocab).to(self.device)\n\n        hidden = self.enc(src)\n        x = trg[0]\n\n        for t in range(1, trg_len):\n            out, hidden = self.dec(x, hidden)\n            outputs[t] = out\n            top1 = out.argmax(1)\n            x = trg[t] if random.random() < teacher_forcing else top1\n\n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:16:11.556872Z","iopub.execute_input":"2026-02-14T06:16:11.557248Z","iopub.status.idle":"2026-02-14T06:16:11.569518Z","shell.execute_reply.started":"2026-02-14T06:16:11.557224Z","shell.execute_reply":"2026-02-14T06:16:11.568931Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel = Seq2Seq(\n    Encoder(INPUT_DIM, 128, 256),\n    Decoder(OUTPUT_DIM, 128, 256),\n    device\n).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:16:11.570410Z","iopub.execute_input":"2026-02-14T06:16:11.570688Z","iopub.status.idle":"2026-02-14T06:16:11.717237Z","shell.execute_reply.started":"2026-02-14T06:16:11.570666Z","shell.execute_reply":"2026-02-14T06:16:11.716481Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"def train(model, loader):\n    model.train()\n    total_loss = 0\n    for b in loader:\n        src = b[\"src\"].transpose(0,1).to(device)\n        trg_in = b[\"trg_in\"].transpose(0,1).to(device)\n        trg_out = b[\"trg_out\"].transpose(0,1).to(device)\n\n        optimizer.zero_grad()\n        output = model(src, trg_in)\n\n        output = output[1:].reshape(-1, output.shape[-1])\n        trg_out = trg_out[1:].reshape(-1)\n\n        loss = criterion(output, trg_out)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for b in loader:\n            src = b[\"src\"].transpose(0,1).to(device)\n            trg_in = b[\"trg_in\"].transpose(0,1).to(device)\n            trg_out = b[\"trg_out\"].transpose(0,1).to(device)\n\n            output = model(src, trg_in, teacher_forcing=0)\n            output = output[1:].reshape(-1, output.shape[-1])\n            trg_out = trg_out[1:].reshape(-1)\n\n            loss = criterion(output, trg_out)\n            total_loss += loss.item()\n    return total_loss / len(loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:16:11.718029Z","iopub.execute_input":"2026-02-14T06:16:11.718231Z","iopub.status.idle":"2026-02-14T06:16:11.725264Z","shell.execute_reply.started":"2026-02-14T06:16:11.718212Z","shell.execute_reply":"2026-02-14T06:16:11.724522Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"best_val_loss = float('inf')\n\nfor epoch in range(10):\n    train_loss = train(model, train_loader)\n    val_loss = evaluate(model, val_loader)\n    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), \"best_model.pt\")\n        print(\"ðŸ’¾ Saved Best Model!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:36:19.029218Z","iopub.execute_input":"2026-02-14T06:36:19.029539Z","iopub.status.idle":"2026-02-14T06:50:04.615863Z","shell.execute_reply.started":"2026-02-14T06:36:19.029510Z","shell.execute_reply":"2026-02-14T06:50:04.615058Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Train Loss = 4.8843, Val Loss = 6.0607\nðŸ’¾ Saved Best Model!\nEpoch 2: Train Loss = 4.8615, Val Loss = 6.2293\nEpoch 3: Train Loss = 4.8751, Val Loss = 5.9968\nðŸ’¾ Saved Best Model!\nEpoch 4: Train Loss = 4.8054, Val Loss = 6.0667\nEpoch 5: Train Loss = 4.7570, Val Loss = 6.1780\nEpoch 6: Train Loss = 4.7393, Val Loss = 6.2079\nEpoch 7: Train Loss = 4.7004, Val Loss = 6.0770\nEpoch 8: Train Loss = 4.6934, Val Loss = 6.1861\nEpoch 9: Train Loss = 4.6961, Val Loss = 6.0962\nEpoch 10: Train Loss = 4.6772, Val Loss = 6.0596\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"def generate_code(model, docstring, max_len=80):\n    model.eval()\n    tokens = tokenizer.encode(docstring, add_special_tokens=False)\n    src = torch.tensor(tokens).unsqueeze(1).to(device)  # [seq_len, batch=1]\n\n    with torch.no_grad():\n        hidden = model.enc(src)\n        x = torch.tensor([SOS_IDX]).to(device)  # [1]\n        output_tokens = []\n\n        for _ in range(max_len):\n            out, hidden = model.dec(x, hidden)  # out: [batch=1, vocab_size]\n            top1 = out.argmax(dim=1).item()      # âœ… argmax over vocab dimension\n            if top1 == EOS_IDX:\n                break\n            output_tokens.append(top1)\n            x = torch.tensor([top1]).to(device)\n\n    return tokenizer.decode(output_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:50:18.108146Z","iopub.status.idle":"2026-02-14T06:50:18.108487Z","shell.execute_reply.started":"2026-02-14T06:50:18.108313Z","shell.execute_reply":"2026-02-14T06:50:18.108346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nsmooth = SmoothingFunction().method1\n\ndef bleu_score(model, dataset, n_samples=100):\n    scores = []\n    for i in range(n_samples):\n        ref_text = dataset[i][\"code\"]\n        doc_text = dataset[i][\"docstring\"]\n\n        # Generate prediction\n        pred_text = generate_code(model, doc_text)\n\n        # Encode both reference and prediction using tokenizer\n        ref_tokens = tokenizer.encode(ref_text, add_special_tokens=False)\n        pred_tokens = tokenizer.encode(pred_text, add_special_tokens=False)\n\n        # Convert to strings for BLEU\n        ref_tokens = [str(tok) for tok in ref_tokens]\n        pred_tokens = [str(tok) for tok in pred_tokens]\n\n        # Compute BLEU score with smoothing\n        scores.append(sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth))\n    return np.mean(scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:59:11.068902Z","iopub.execute_input":"2026-02-14T06:59:11.069410Z","iopub.status.idle":"2026-02-14T06:59:11.074785Z","shell.execute_reply.started":"2026-02-14T06:59:11.069385Z","shell.execute_reply":"2026-02-14T06:59:11.074129Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load(\"best_model.pt\"))\nmodel.to(device)\n\nprint(\"BLEU score on 100 test examples:\", bleu_score(model, test_data, n_samples=100))\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"BLEU score on 100 test examples: 0.021226243711369244\n","output_type":"stream"}],"execution_count":49}]}