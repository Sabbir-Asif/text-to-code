{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets tokenizers nltk sacrebleu transformers rouge-score -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:50:43.989106Z","iopub.execute_input":"2026-02-15T09:50:43.989821Z","iopub.status.idle":"2026-02-15T09:50:48.519195Z","shell.execute_reply.started":"2026-02-15T09:50:43.989791Z","shell.execute_reply":"2026-02-15T09:50:48.518270Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport numpy as np\nimport random\nimport os\nfrom tqdm import tqdm\nimport math\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:50:48.521046Z","iopub.execute_input":"2026-02-15T09:50:48.521315Z","iopub.status.idle":"2026-02-15T09:50:49.801922Z","shell.execute_reply.started":"2026-02-15T09:50:48.521289Z","shell.execute_reply":"2026-02-15T09:50:49.801321Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class Config:\n    # Model Architecture\n    EMBED_SIZE = 256\n    HIDDEN_SIZE = 256\n    NUM_LAYERS = 2\n    DROPOUT = 0.3\n    BIDIRECTIONAL = False  # optional True if you reduce HIDDEN_SIZE\n    \n    # Training\n    BATCH_SIZE = 32\n    EPOCHS = 20\n    LR = 0.001\n    CLIP = 1.0\n    TEACHER_FORCING_RATIO = 0.7\n    TEACHER_FORCING_DECAY = 0.95\n    \n    # Data\n    MAX_DOC_LEN = 60\n    MAX_CODE_LEN = 120\n    VOCAB_SIZE = 10000\n    \n    # Optimization\n    WEIGHT_DECAY = 1e-5\n    LR_SCHEDULER_PATIENCE = 2\n    LR_SCHEDULER_FACTOR = 0.5\n    \n    # Generation\n    BEAM_SIZE = 5\n    LENGTH_PENALTY = 1.0\n    \n    # System\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    SEED = 42\n\nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:50:49.802806Z","iopub.execute_input":"2026-02-15T09:50:49.803288Z","iopub.status.idle":"2026-02-15T09:50:49.861330Z","shell.execute_reply.started":"2026-02-15T09:50:49.803264Z","shell.execute_reply":"2026-02-15T09:50:49.860513Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Set seeds for reproducibility\ntorch.manual_seed(config.SEED)\nrandom.seed(config.SEED)\nnp.random.seed(config.SEED)\n\nprint(f\"Using device: {config.DEVICE}\")\nprint(f\"Model will have {config.NUM_LAYERS} layers with hidden size {config.HIDDEN_SIZE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:50:49.862324Z","iopub.execute_input":"2026-02-15T09:50:49.863057Z","iopub.status.idle":"2026-02-15T09:50:49.878793Z","shell.execute_reply.started":"2026-02-15T09:50:49.863032Z","shell.execute_reply":"2026-02-15T09:50:49.878171Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nModel will have 2 layers with hidden size 256\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(\"\\nüìä Loading dataset...\")\ndataset = load_dataset(\"Nan-Do/code-search-net-python\")\n\n# Use more data for better performance\nfull_data = dataset[\"train\"].select(range(8000))\n\nsplit1 = full_data.train_test_split(test_size=0.15, seed=config.SEED)\ntrain_data = split1[\"train\"]\ntemp_data = split1[\"test\"]\n\nsplit2 = temp_data.train_test_split(test_size=0.5, seed=config.SEED)\nval_data = split2[\"train\"]\ntest_data = split2[\"test\"]\n\nprint(f\"‚úÖ Train: {len(train_data):,} | Val: {len(val_data):,} | Test: {len(test_data):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:50:49.880638Z","iopub.execute_input":"2026-02-15T09:50:49.880854Z","iopub.status.idle":"2026-02-15T09:51:02.767372Z","shell.execute_reply.started":"2026-02-15T09:50:49.880834Z","shell.execute_reply":"2026-02-15T09:51:02.766696Z"}},"outputs":[{"name":"stdout","text":"\nüìä Loading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"144866bb83be49d2b20bbfe40ac4cb7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00004-ee77a7de79eb2a(‚Ä¶):   0%|          | 0.00/155M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60785091faac463181b64c8add9c564b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00004-648b3bede2edf6(‚Ä¶):   0%|          | 0.00/139M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"175ca1a390a04fbba51a44ea1f266c15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00002-of-00004-1dfd72b171e6b2(‚Ä¶):   0%|          | 0.00/153M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bd49707212e43cc82b02003171c2c72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00003-of-00004-184ab6d0e3c690(‚Ä¶):   0%|          | 0.00/151M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b9cb4b3f4b24d889e65d9e84129f748"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/455243 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f9e3337d304f19b7f29e7d07c98562"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Train: 6,800 | Val: 600 | Test: 600\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def create_advanced_tokenizer(train_data, vocab_size=10000):\n    \"\"\"Create a robust BPE tokenizer with proper configuration\"\"\"\n    tokenizer_path = f\"tokenizer_v{vocab_size}.json\"\n    \n    if os.path.exists(tokenizer_path):\n        print(\"üìÇ Loading existing tokenizer...\")\n        tokenizer = Tokenizer.from_file(tokenizer_path)\n    else:\n        print(\"üîß Training new tokenizer...\")\n        # Initialize BPE tokenizer\n        tokenizer = Tokenizer(models.BPE(unk_token=\"<UNK>\"))\n        \n        # Use byte-level pre-tokenizer for better coverage\n        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n        tokenizer.decoder = decoders.ByteLevel()\n        \n        # Special tokens\n        special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\", \"<MASK>\"]\n        \n        trainer = trainers.BpeTrainer(\n            vocab_size=vocab_size,\n            special_tokens=special_tokens,\n            min_frequency=2,\n            show_progress=True\n        )\n        \n        # Prepare corpus\n        corpus = []\n        for i in range(min(10000, len(train_data))):\n            corpus.append(train_data[i][\"docstring\"] if train_data[i][\"docstring\"] else \"\")\n            corpus.append(train_data[i][\"code\"] if train_data[i][\"code\"] else \"\")\n        \n        tokenizer.train_from_iterator(corpus, trainer)\n        tokenizer.save(tokenizer_path)\n        print(f\"‚úÖ Tokenizer saved with vocab size: {tokenizer.get_vocab_size()}\")\n    \n    return tokenizer\n\ntokenizer = create_advanced_tokenizer(train_data, config.VOCAB_SIZE)\n# Token helpers\nPAD_IDX = tokenizer.token_to_id(\"<PAD>\")\nSOS_IDX = tokenizer.token_to_id(\"<SOS>\")\nEOS_IDX = tokenizer.token_to_id(\"<EOS>\")\nUNK_IDX = tokenizer.token_to_id(\"<UNK>\")\nVOCAB_SIZE = tokenizer.get_vocab_size()\n\nprint(f\"üìù Vocab: {VOCAB_SIZE} | PAD: {PAD_IDX} | SOS: {SOS_IDX} | EOS: {EOS_IDX}\")\ndef encode(text):\n    \"\"\"Encode text with proper error handling\"\"\"\n    if not text:\n        return []\n    try:\n        return tokenizer.encode(text).ids\n    except:\n        return [UNK_IDX]\n\ndef decode(ids):\n    \"\"\"Decode ids with proper error handling\"\"\"\n    if not ids:\n        return \"\"\n    try:\n        return tokenizer.decode(ids)\n    except:\n        return \"<DECODE_ERROR>\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:02.768411Z","iopub.execute_input":"2026-02-15T09:51:02.769031Z","iopub.status.idle":"2026-02-15T09:51:02.811181Z","shell.execute_reply.started":"2026-02-15T09:51:02.769009Z","shell.execute_reply":"2026-02-15T09:51:02.810347Z"}},"outputs":[{"name":"stdout","text":"üìÇ Loading existing tokenizer...\nüìù Vocab: 10000 | PAD: 0 | SOS: 1 | EOS: 2\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class CodeDataset(Dataset):\n    def __init__(self, data, max_doc_len, max_code_len, augment=False):\n        self.data = data\n        self.max_doc_len = max_doc_len\n        self.max_code_len = max_code_len\n        self.augment = augment\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Extract and clean text\n        docstring = str(item[\"docstring\"]).strip() if item[\"docstring\"] else \"\"\n        code = str(item[\"code\"]).strip() if item[\"code\"] else \"\"\n        \n        # Data augmentation for training\n        if self.augment and random.random() < 0.1:\n            # Randomly drop some characters (simulates noise)\n            if len(docstring) > 10:\n                drop_idx = random.randint(0, len(docstring)-1)\n                docstring = docstring[:drop_idx] + docstring[drop_idx+1:]\n        \n        # Encode\n        doc_ids = encode(docstring)[:self.max_doc_len]\n        code_ids = encode(code)[:self.max_code_len]\n        \n        # Ensure minimum length\n        if len(doc_ids) == 0:\n            doc_ids = [PAD_IDX]\n        if len(code_ids) == 0:\n            code_ids = [PAD_IDX]\n            \n        return {\n            \"doc\": doc_ids,\n            \"code\": code_ids,\n            \"doc_len\": len(doc_ids),\n            \"code_len\": len(code_ids)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:02.812297Z","iopub.execute_input":"2026-02-15T09:51:02.812628Z","iopub.status.idle":"2026-02-15T09:51:02.826352Z","shell.execute_reply.started":"2026-02-15T09:51:02.812606Z","shell.execute_reply":"2026-02-15T09:51:02.825492Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def advanced_collate_fn(batch):\n    \"\"\"Advanced collate function with sequence lengths\"\"\"\n    docs = [b[\"doc\"] for b in batch]\n    codes = [b[\"code\"] for b in batch]\n    doc_lens = [b[\"doc_len\"] for b in batch]\n    code_lens = [b[\"code_len\"] for b in batch]\n    \n    max_doc = max(doc_lens)\n    max_code = max(code_lens)\n    \n    src_list = []\n    trg_in_list = []\n    trg_out_list = []\n    \n    for d, c, d_len, c_len in zip(docs, codes, doc_lens, code_lens):\n        # Source padding\n        src_list.append(d + [PAD_IDX] * (max_doc - d_len))\n        \n        # Target input (with SOS)\n        trg_in_list.append([SOS_IDX] + c + [PAD_IDX] * (max_code - c_len))\n        \n        # Target output (with EOS)\n        trg_out_list.append(c + [EOS_IDX] + [PAD_IDX] * (max_code - c_len))\n    \n    return {\n        \"src\": torch.tensor(src_list, dtype=torch.long),\n        \"trg_in\": torch.tensor(trg_in_list, dtype=torch.long),\n        \"trg_out\": torch.tensor(trg_out_list, dtype=torch.long),\n        \"src_len\": torch.tensor(doc_lens, dtype=torch.long),\n        \"trg_len\": torch.tensor(code_lens, dtype=torch.long)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:02.827444Z","iopub.execute_input":"2026-02-15T09:51:02.827666Z","iopub.status.idle":"2026-02-15T09:51:02.841616Z","shell.execute_reply.started":"2026-02-15T09:51:02.827647Z","shell.execute_reply":"2026-02-15T09:51:02.840935Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Create datasets with augmentation for training\ntrain_dataset = CodeDataset(train_data, config.MAX_DOC_LEN, config.MAX_CODE_LEN, augment=True)\nval_dataset = CodeDataset(val_data, config.MAX_DOC_LEN, config.MAX_CODE_LEN, augment=False)\ntest_dataset = CodeDataset(test_data, config.MAX_DOC_LEN, config.MAX_CODE_LEN, augment=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:02.842628Z","iopub.execute_input":"2026-02-15T09:51:02.843140Z","iopub.status.idle":"2026-02-15T09:51:02.856823Z","shell.execute_reply.started":"2026-02-15T09:51:02.843119Z","shell.execute_reply":"2026-02-15T09:51:02.856200Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# DataLoaders\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=config.BATCH_SIZE, \n    shuffle=True, \n    collate_fn=advanced_collate_fn,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=config.BATCH_SIZE, \n    collate_fn=advanced_collate_fn,\n    num_workers=2,\n    pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=config.BATCH_SIZE, \n    collate_fn=advanced_collate_fn,\n    num_workers=2,\n    pin_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:02.857797Z","iopub.execute_input":"2026-02-15T09:51:02.858093Z","iopub.status.idle":"2026-02-15T09:51:02.868424Z","shell.execute_reply.started":"2026-02-15T09:51:02.858064Z","shell.execute_reply":"2026-02-15T09:51:02.867822Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Attention(nn.Module):\n    \"\"\"Combined attention mechanism with multiple scoring functions\"\"\"\n    def __init__(self, hidden_size, method='general'):\n        super().__init__()\n        self.method = method\n        self.hidden_size = hidden_size\n        \n        if method == 'general':\n            self.attn = nn.Linear(hidden_size, hidden_size)\n        elif method == 'concat':\n            self.attn = nn.Linear(hidden_size * 2, hidden_size)\n            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n        elif method == 'dot':\n            pass  # No parameters needed\n        else:\n            raise ValueError(f'Unknown attention method: {method}')\n            \n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        if hasattr(self, 'attn'):\n            nn.init.xavier_uniform_(self.attn.weight)\n        if hasattr(self, 'v'):\n            nn.init.xavier_uniform_(self.v)\n    \n    def forward(self, hidden, encoder_outputs, mask=None):\n        \"\"\"\n        hidden: (batch_size, hidden_size * num_directions)\n        encoder_outputs: (batch_size, seq_len, hidden_size * num_directions)\n        mask: (batch_size, seq_len)\n        \"\"\"\n        batch_size, seq_len = encoder_outputs.shape[0], encoder_outputs.shape[1]\n        \n        # Calculate attention energies\n        if self.method == 'dot':\n            # Dot product attention\n            hidden = hidden.unsqueeze(2)  # (batch, hidden, 1)\n            energy = torch.bmm(encoder_outputs, hidden).squeeze(2)  # (batch, seq_len)\n            \n        elif self.method == 'general':\n            # General attention\n            energy = self.attn(encoder_outputs)  # (batch, seq_len, hidden)\n            hidden = hidden.unsqueeze(2)  # (batch, hidden, 1)\n            energy = torch.bmm(energy, hidden).squeeze(2)  # (batch, seq_len)\n            \n        else:  # concat\n            # Concatenation attention\n            hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # (batch, seq_len, hidden)\n            energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n            energy = energy @ self.v.t()  # (batch, seq_len, 1)\n            energy = energy.squeeze(2)  # (batch, seq_len)\n        \n        # Apply mask if provided\n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -1e10)\n        \n        # Normalize\n        attention_weights = torch.softmax(energy, dim=1)\n        \n        # Apply attention\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n        \n        return context, attention_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:02.869324Z","iopub.execute_input":"2026-02-15T09:51:02.869561Z","iopub.status.idle":"2026-02-15T09:51:02.880973Z","shell.execute_reply.started":"2026-02-15T09:51:02.869541Z","shell.execute_reply":"2026-02-15T09:51:02.880242Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, bidirectional):\n        super().__init__()\n        \n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        self.num_directions = 2 if bidirectional else 1\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_IDX)\n        self.embedding_dropout = nn.Dropout(dropout)\n        \n        # LSTM\n        self.rnn = nn.LSTM(\n            embed_size,\n            hidden_size,\n            num_layers,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=bidirectional,\n            batch_first=True\n        )\n        \n        # Layer normalization\n        self.layer_norm = nn.LayerNorm(hidden_size * self.num_directions)\n        \n        # Initialize weights\n        self.init_weights()\n    \n    def init_weights(self):\n        for name, param in self.rnn.named_parameters():\n            if 'weight_ih' in name:\n                nn.init.xavier_uniform_(param)\n            elif 'weight_hh' in name:\n                nn.init.orthogonal_(param)\n            elif 'bias' in name:\n                nn.init.zeros_(param)\n                # Set forget gate bias to 1\n                n = param.size(0)\n                param.data[n//4:n//2].fill_(1.0)\n    \n    def forward(self, src, src_len):\n        \"\"\"\n        src: (batch_size, seq_len)\n        src_len: (batch_size)\n        \"\"\"\n        # Embedding\n        embedded = self.embedding(src)\n        embedded = self.embedding_dropout(embedded)\n        \n        # Pack sequences\n        packed_embedded = pack_padded_sequence(\n            embedded, \n            src_len.cpu(), \n            batch_first=True, \n            enforce_sorted=False\n        )\n        \n        # RNN forward\n        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)\n        \n        # Unpack\n        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n        \n        # Apply layer norm\n        outputs = self.layer_norm(outputs)\n        \n        # Handle bidirectional hidden states\n        if self.bidirectional:\n            # Reshape hidden: (num_layers * num_directions, batch, hidden_size)\n            # to: (num_layers, batch, hidden_size * num_directions)\n            hidden = hidden.view(self.num_layers, self.num_directions, \n                               hidden.size(1), self.hidden_size)\n            hidden = hidden.transpose(1, 2).contiguous()\n            hidden = hidden.view(self.num_layers, hidden.size(2), \n                               self.hidden_size * self.num_directions)\n            \n            cell = cell.view(self.num_layers, self.num_directions,\n                           cell.size(1), self.hidden_size)\n            cell = cell.transpose(1, 2).contiguous()\n            cell = cell.view(self.num_layers, cell.size(2), \n                           self.hidden_size * self.num_directions)\n        \n        return outputs, hidden, cell\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:02.881849Z","iopub.execute_input":"2026-02-15T09:51:02.882129Z","iopub.status.idle":"2026-02-15T09:51:02.899775Z","shell.execute_reply.started":"2026-02-15T09:51:02.882110Z","shell.execute_reply":"2026-02-15T09:51:02.899180Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, attention_method='general'):\n        super().__init__()\n        \n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_IDX)\n        self.embedding_dropout = nn.Dropout(dropout)\n        \n        # Attention\n        self.attention = Attention(hidden_size, method=attention_method)\n        \n        # Input feeding: combine embedding with previous attention context\n        self.input_projection = nn.Linear(embed_size + hidden_size, embed_size)\n        \n        # LSTM\n        self.rnn = nn.LSTM(\n            embed_size,\n            hidden_size,\n            num_layers,\n            dropout=dropout if num_layers > 1 else 0,\n            batch_first=True\n        )\n        \n        # Output projection\n        self.output_projection = nn.Sequential(\n            nn.Linear(hidden_size * 2, hidden_size),\n            nn.Tanh(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, vocab_size)\n        )\n        \n        # Layer normalization\n        self.layer_norm1 = nn.LayerNorm(hidden_size)\n        self.layer_norm2 = nn.LayerNorm(hidden_size * 2)\n        \n        # Initialize weights\n        self.init_weights()\n    \n    def init_weights(self):\n        for name, param in self.rnn.named_parameters():\n            if 'weight_ih' in name:\n                nn.init.xavier_uniform_(param)\n            elif 'weight_hh' in name:\n                nn.init.orthogonal_(param)\n        \n        # Initialize output projection\n        for layer in self.output_projection:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)\n                nn.init.zeros_(layer.bias)\n    \n    def forward(self, x, hidden, cell, encoder_outputs, mask=None, prev_context=None):\n        \"\"\"\n        x: (batch_size)\n        hidden: (num_layers, batch_size, hidden_size)\n        cell: (num_layers, batch_size, hidden_size)\n        encoder_outputs: (batch_size, seq_len, hidden_size)\n        prev_context: (batch_size, hidden_size)\n        \"\"\"\n        batch_size = x.size(0)\n        \n        # Embedding\n        embedded = self.embedding(x.unsqueeze(1))  # (batch, 1, embed)\n        embedded = self.embedding_dropout(embedded)\n        \n        # Input feeding: combine with previous context\n        if prev_context is not None:\n            embedded = torch.cat((embedded.squeeze(1), prev_context), dim=1)  # (batch, embed + hidden)\n            embedded = self.input_projection(embedded)  # (batch, embed)\n            embedded = embedded.unsqueeze(1)  # (batch, 1, embed)\n        else:\n            embedded = embedded\n        \n        # RNN forward\n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        output = output.squeeze(1)  # (batch, hidden)\n        output = self.layer_norm1(output)\n        \n        # Attention\n        context, attention_weights = self.attention(output, encoder_outputs, mask)\n        \n        # Combine output and context\n        combined = torch.cat((output, context), dim=1)  # (batch, hidden * 2)\n        combined = self.layer_norm2(combined)\n        \n        # Generate prediction\n        prediction = self.output_projection(combined)  # (batch, vocab_size)\n        \n        return prediction, hidden, cell, context, attention_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:02.900594Z","iopub.execute_input":"2026-02-15T09:51:02.900853Z","iopub.status.idle":"2026-02-15T09:51:02.916539Z","shell.execute_reply.started":"2026-02-15T09:51:02.900825Z","shell.execute_reply":"2026-02-15T09:51:02.915765Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device, config):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        self.config = config\n        \n    def forward(self, src, trg, src_len, trg_len, teacher_forcing_ratio=0.5):\n        \"\"\"\n        src: (batch_size, src_seq_len)\n        trg: (batch_size, trg_seq_len)\n        src_len: (batch_size)\n        trg_len: (batch_size)\n        \"\"\"\n        batch_size = src.size(0)\n        trg_seq_len = trg.size(1)\n        vocab_size = self.decoder.vocab_size\n        \n        # Create mask for encoder outputs\n        mask = (src != PAD_IDX).float()\n        \n        # Encoder\n        encoder_outputs, hidden, cell = self.encoder(src, src_len)\n        \n        # Initialize decoder\n        decoder_input = trg[:, 0]  # SOS token\n        decoder_context = None\n        \n        # Store outputs\n        outputs = torch.zeros(batch_size, trg_seq_len, vocab_size).to(self.device)\n        attentions = torch.zeros(batch_size, trg_seq_len, src.size(1)).to(self.device)\n        \n        for t in range(1, trg_seq_len):\n            decoder_output, hidden, cell, decoder_context, attention = self.decoder(\n                decoder_input, hidden, cell, encoder_outputs, mask, decoder_context\n            )\n            \n            outputs[:, t] = decoder_output\n            attentions[:, t] = attention\n            \n            # Teacher forcing\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = decoder_output.argmax(1)\n            decoder_input = trg[:, t] if teacher_force else top1\n        \n        return outputs, attentions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:02.918878Z","iopub.execute_input":"2026-02-15T09:51:02.919473Z","iopub.status.idle":"2026-02-15T09:51:02.933244Z","shell.execute_reply.started":"2026-02-15T09:51:02.919451Z","shell.execute_reply":"2026-02-15T09:51:02.932528Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def train_epoch(model, loader, optimizer, criterion, config, epoch):\n    model.train()\n    total_loss = 0\n    total_tokens = 0\n    \n    # Decay teacher forcing\n    teacher_forcing = config.TEACHER_FORCING_RATIO * (config.TEACHER_FORCING_DECAY ** epoch)\n    \n    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1} [Train]\")\n    for batch in progress_bar:\n        # Move to device\n        src = batch[\"src\"].to(config.DEVICE)\n        trg_in = batch[\"trg_in\"].to(config.DEVICE)\n        trg_out = batch[\"trg_out\"].to(config.DEVICE)\n        src_len = batch[\"src_len\"].to(config.DEVICE)\n        trg_len = batch[\"trg_len\"].to(config.DEVICE)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs, _ = model(src, trg_in, src_len, trg_len, teacher_forcing)\n        \n        # Reshape for loss calculation\n        outputs = outputs[:, 1:].reshape(-1, outputs.shape[-1])\n        trg_out = trg_out[:, 1:].reshape(-1)\n        \n        # Calculate loss\n        loss = criterion(outputs, trg_out)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config.CLIP)\n        \n        optimizer.step()\n        \n        # Update metrics\n        non_pad = (trg_out != PAD_IDX).sum().item()\n        total_loss += loss.item() * non_pad\n        total_tokens += non_pad\n        \n        # Update progress bar\n        progress_bar.set_postfix({\n            'loss': f\"{loss.item():.4f}\",\n            'teacher': f\"{teacher_forcing:.2f}\"\n        })\n    \n    return total_loss / total_tokens if total_tokens > 0 else 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:02.934105Z","iopub.execute_input":"2026-02-15T09:51:02.934553Z","iopub.status.idle":"2026-02-15T09:51:02.949118Z","shell.execute_reply.started":"2026-02-15T09:51:02.934515Z","shell.execute_reply":"2026-02-15T09:51:02.948392Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def validate_epoch(model, loader, criterion, config):\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    \n    with torch.no_grad():\n        progress_bar = tqdm(loader, desc=\"[Validate]\")\n        for batch in progress_bar:\n            src = batch[\"src\"].to(config.DEVICE)\n            trg_in = batch[\"trg_in\"].to(config.DEVICE)\n            trg_out = batch[\"trg_out\"].to(config.DEVICE)\n            src_len = batch[\"src_len\"].to(config.DEVICE)\n            trg_len = batch[\"trg_len\"].to(config.DEVICE)\n            \n            # Forward pass (no teacher forcing)\n            outputs, _ = model(src, trg_in, src_len, trg_len, teacher_forcing_ratio=0)\n            \n            outputs = outputs[:, 1:].reshape(-1, outputs.shape[-1])\n            trg_out = trg_out[:, 1:].reshape(-1)\n            \n            loss = criterion(outputs, trg_out)\n            \n            non_pad = (trg_out != PAD_IDX).sum().item()\n            total_loss += loss.item() * non_pad\n            total_tokens += non_pad\n            \n            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n    \n    return total_loss / total_tokens if total_tokens > 0 else 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:02.950027Z","iopub.execute_input":"2026-02-15T09:51:02.950240Z","iopub.status.idle":"2026-02-15T09:51:02.959498Z","shell.execute_reply.started":"2026-02-15T09:51:02.950220Z","shell.execute_reply":"2026-02-15T09:51:02.958945Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def beam_search_decode(model, src, src_len, beam_size=5, max_len=100, length_penalty=1.0):\n    \"\"\"Beam search decoding for batch_first=True encoder and decoder\"\"\"\n    with torch.no_grad():\n        # src shape: (batch_size=1, seq_len)\n        # src_len shape: (1,)\n        \n        # Encode source\n        encoder_outputs, hidden, cell = model.encoder(src, src_len)\n        # encoder_outputs: (1, seq_len, hidden_size)\n        # hidden: (num_layers, 1, hidden_size)\n        # cell: (num_layers, 1, hidden_size)\n        \n        # Initialize beam with start token\n        start_token = torch.tensor([SOS_IDX]).to(config.DEVICE)\n        \n        # Initial hypotheses: (sequence, log_prob, hidden, cell, context)\n        hypotheses = [(start_token, 0.0, hidden, cell, None)]\n        \n        # Create source mask\n        mask = (src != PAD_IDX).float()  # (1, seq_len)\n        \n        # Beam search\n        for step in range(max_len):\n            all_candidates = []\n            \n            for seq, score, h, c, ctx in hypotheses:\n                # Check if sequence ended\n                if seq[-1].item() == EOS_IDX:\n                    all_candidates.append((seq, score, h, c, ctx))\n                    continue\n                \n                # Prepare decoder input - current token\n                x = seq[-1].unsqueeze(0)  # (1,)\n                \n                # Decode one step\n                output, h_new, c_new, ctx_new, _ = model.decoder(\n                    x,  # (1,)\n                    h,  # (num_layers, 1, hidden_size)\n                    c,  # (num_layers, 1, hidden_size)\n                    encoder_outputs,  # (1, seq_len, hidden_size)\n                    mask,  # (1, seq_len)\n                    ctx  # (1, hidden_size) or None\n                )\n                # output: (1, vocab_size)\n                \n                # Get top-k tokens\n                log_probs = torch.log_softmax(output, dim=-1)  # (1, vocab_size)\n                topk_log_probs, topk_tokens = log_probs.topk(min(beam_size, log_probs.size(-1)))\n                \n                for k in range(beam_size):\n                    token = topk_tokens[0, k].unsqueeze(0)  # (1,)\n                    log_prob = topk_log_probs[0, k].item()\n                    new_seq = torch.cat([seq, token])\n                    \n                    # Calculate score with length penalty\n                    if length_penalty > 0:\n                        # Normalize by sequence length\n                        new_score = (score * step + log_prob) / (step + 1)\n                    else:\n                        new_score = score + log_prob\n                    \n                    all_candidates.append((new_seq, new_score, h_new, c_new, ctx_new))\n            \n            # Select top-k hypotheses\n            all_candidates.sort(key=lambda x: x[1], reverse=True)\n            hypotheses = all_candidates[:beam_size]\n            \n            # Early stopping if all have EOS\n            if all(h[0][-1].item() == EOS_IDX for h in hypotheses):\n                break\n        \n        # Return best sequence (excluding SOS)\n        best_seq = hypotheses[0][0]\n        return best_seq[1:] if len(best_seq) > 1 else best_seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:50:28.149495Z","iopub.execute_input":"2026-02-15T10:50:28.150066Z","iopub.status.idle":"2026-02-15T10:50:28.159557Z","shell.execute_reply.started":"2026-02-15T10:50:28.150041Z","shell.execute_reply":"2026-02-15T10:50:28.158822Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"def generate_code(model, docstring, use_beam_search=True):\n    \"\"\"Generate code with optional beam search\"\"\"\n    model.eval()\n    \n    # Encode docstring\n    tokens = encode(docstring)[:config.MAX_DOC_LEN]\n    if len(tokens) == 0:\n        tokens = [PAD_IDX]\n    \n    # Convert to tensor - for batch_first=True encoder\n    src = torch.tensor(tokens, dtype=torch.long).to(config.DEVICE)\n    \n    # Correct shapes for encoder (batch_first=True)\n    src = src.unsqueeze(0)  # (batch_size=1, seq_len)\n    src_len = torch.tensor([src.size(1)], dtype=torch.long).to(config.DEVICE)\n    \n    if use_beam_search:\n        # Beam search decoding\n        best_seq = beam_search_decode(\n            model,\n            src,  # (1, seq_len)\n            src_len,  # (1,)\n            beam_size=config.BEAM_SIZE,\n            max_len=config.MAX_CODE_LEN,\n            length_penalty=config.LENGTH_PENALTY\n        )\n        \n        # Convert to list of tokens\n        if isinstance(best_seq, torch.Tensor):\n            outputs = best_seq.tolist()\n        else:\n            outputs = best_seq\n    \n    else:\n        # Greedy decoding\n        with torch.no_grad():\n            encoder_outputs, hidden, cell = model.encoder(src, src_len)\n            \n            x = torch.tensor([SOS_IDX]).to(config.DEVICE)\n            outputs = []\n            context = None\n            \n            # Create mask\n            mask = (src != PAD_IDX).float()  # (1, seq_len)\n            \n            for _ in range(config.MAX_CODE_LEN):\n                output, hidden, cell, context, _ = model.decoder(\n                    x,  # (1,)\n                    hidden,  # (num_layers, 1, hidden_size)\n                    cell,   # (num_layers, 1, hidden_size)\n                    encoder_outputs,  # (1, seq_len, hidden_size)\n                    mask,   # (1, seq_len)\n                    context  # (1, hidden_size) or None\n                )\n                \n                top1 = output.argmax(1).item()\n                \n                if top1 == EOS_IDX:\n                    break\n                    \n                outputs.append(top1)\n                x = torch.tensor([top1]).to(config.DEVICE)\n    \n    return decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:50:32.632035Z","iopub.execute_input":"2026-02-15T10:50:32.632741Z","iopub.status.idle":"2026-02-15T10:50:32.640309Z","shell.execute_reply.started":"2026-02-15T10:50:32.632714Z","shell.execute_reply":"2026-02-15T10:50:32.639303Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def calculate_bleu(model, test_data, n_samples=200, use_beam_search=True):\n    \"\"\"Calculate BLEU score with proper tokenization\"\"\"\n    from nltk.translate.bleu_score import SmoothingFunction\n    import numpy as np\n    from tqdm import tqdm\n    \n    smooth = SmoothingFunction().method4\n    scores = []\n    \n    for i in tqdm(range(min(n_samples, len(test_data))), desc=\"Calculating BLEU\"):\n        ref = test_data[i][\"code\"]\n        doc = test_data[i][\"docstring\"]\n        \n        if not ref or not doc:\n            continue\n            \n        pred = generate_code(model, doc, use_beam_search)\n        \n        # Proper tokenization for BLEU\n        ref_tokens = tokenizer.encode(ref).tokens\n        pred_tokens = tokenizer.encode(pred).tokens\n        \n        if len(ref_tokens) > 0 and len(pred_tokens) > 0:\n            try:\n                score = sentence_bleu([ref_tokens], pred_tokens, \n                                    smoothing_function=smooth)\n                scores.append(score)\n            except:\n                continue\n    \n    mean_score = np.mean(scores) if scores else 0.0\n    return mean_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:55:26.946462Z","iopub.execute_input":"2026-02-15T10:55:26.946717Z","iopub.status.idle":"2026-02-15T10:55:26.952995Z","shell.execute_reply.started":"2026-02-15T10:55:26.946697Z","shell.execute_reply":"2026-02-15T10:55:26.952374Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"def calculate_exact_match(model, test_data, n_samples=200):\n    \"\"\"Calculate exact match accuracy\"\"\"\n    matches = 0\n    total = 0\n    \n    for i in tqdm(range(min(n_samples, len(test_data))), desc=\"Calculating EM\"):\n        ref = test_data[i][\"code\"].strip()\n        doc = test_data[i][\"docstring\"]\n        \n        if not ref or not doc:\n            continue\n            \n        pred = generate_code(model, doc, use_beam_search=True)\n        \n        # Normalize for comparison\n        ref_norm = ' '.join(ref.split())\n        pred_norm = ' '.join(pred.split())\n        \n        if ref_norm == pred_norm:\n            matches += 1\n        total += 1\n    \n    return matches / total if total > 0 else 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:03.005189Z","iopub.execute_input":"2026-02-15T09:51:03.005395Z","iopub.status.idle":"2026-02-15T09:51:03.019917Z","shell.execute_reply.started":"2026-02-15T09:51:03.005378Z","shell.execute_reply":"2026-02-15T09:51:03.019138Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"print(\"\\nüöÄ Initializing model...\")\n\nencoder = Encoder(\n    vocab_size=VOCAB_SIZE,\n    embed_size=config.EMBED_SIZE,\n    hidden_size=config.HIDDEN_SIZE,\n    num_layers=config.NUM_LAYERS,\n    dropout=config.DROPOUT,\n    bidirectional=config.BIDIRECTIONAL\n).to(config.DEVICE)\n\ndecoder = Decoder(\n    vocab_size=VOCAB_SIZE,\n    embed_size=config.EMBED_SIZE,\n    hidden_size=config.HIDDEN_SIZE * (2 if config.BIDIRECTIONAL else 1),\n    num_layers=config.NUM_LAYERS,\n    dropout=config.DROPOUT,\n    attention_method='general'\n).to(config.DEVICE)\n\nmodel = Seq2Seq(encoder, decoder, config.DEVICE, config).to(config.DEVICE)\n\n# parameter count\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"üìä Total parameters: {total_params:,}\")\nprint(f\"üìä Trainable parameters: {trainable_params:,}\")\n\n# optimizer\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=config.LR,\n    weight_decay=config.WEIGHT_DECAY\n)\n\n# scheduler (FIXED)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=config.LR_SCHEDULER_FACTOR,\n    patience=config.LR_SCHEDULER_PATIENCE\n)\n\n# loss\ncriterion = nn.CrossEntropyLoss(\n    ignore_index=PAD_IDX,\n    label_smoothing=0.1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:03.020994Z","iopub.execute_input":"2026-02-15T09:51:03.021282Z","iopub.status.idle":"2026-02-15T09:51:06.582240Z","shell.execute_reply.started":"2026-02-15T09:51:03.021255Z","shell.execute_reply":"2026-02-15T09:51:06.581674Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ Initializing model...\nüìä Total parameters: 10,125,840\nüìä Trainable parameters: 10,125,840\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Training loop\nprint(\"\\nüéØ Starting training...\")\nbest_val_loss = float('inf')\npatience_counter = 0\nearly_stopping_patience = 5\n\nfor epoch in range(config.EPOCHS):\n    # Train\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, config, epoch)\n    \n    # Validate\n    val_loss = validate_epoch(model, val_loader, criterion, config)\n    \n    # Update learning rate\n    scheduler.step(val_loss)\n    \n    print(f\"\\nüìà Epoch {epoch+1}/{config.EPOCHS}\")\n    print(f\"   Train Loss: {train_loss:.4f}\")\n    print(f\"   Val Loss: {val_loss:.4f}\")\n    print(f\"   LR: {optimizer.param_groups[0]['lr']:.6f}\")\n    \n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n            'config': config\n        }, \"best_model_attention.pt\")\n        print(f\"‚úÖ Saved best model with val loss: {val_loss:.4f}\")\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        if patience_counter >= early_stopping_patience:\n            print(\"üõë Early stopping triggered\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T09:51:06.583101Z","iopub.execute_input":"2026-02-15T09:51:06.583607Z","iopub.status.idle":"2026-02-15T10:26:45.872782Z","shell.execute_reply.started":"2026-02-15T09:51:06.583584Z","shell.execute_reply":"2026-02-15T10:26:45.871956Z"}},"outputs":[{"name":"stdout","text":"\nüéØ Starting training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:46<00:00,  2.00it/s, loss=7.0653, teacher=0.70]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.43it/s, loss=7.1701]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 1/20\n   Train Loss: 7.0944\n   Val Loss: 7.2440\n   LR: 0.001000\n‚úÖ Saved best model with val loss: 7.2440\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:44<00:00,  2.04it/s, loss=6.5391, teacher=0.66]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.32it/s, loss=7.3326]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 2/20\n   Train Loss: 6.6521\n   Val Loss: 7.3503\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:43<00:00,  2.05it/s, loss=6.4853, teacher=0.63]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.43it/s, loss=7.0091]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 3/20\n   Train Loss: 6.3720\n   Val Loss: 6.9691\n   LR: 0.001000\n‚úÖ Saved best model with val loss: 6.9691\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:43<00:00,  2.06it/s, loss=5.8394, teacher=0.60]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.49it/s, loss=7.2262]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 4/20\n   Train Loss: 6.1826\n   Val Loss: 7.0533\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:42<00:00,  2.08it/s, loss=6.1380, teacher=0.57]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.46it/s, loss=6.9704]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 5/20\n   Train Loss: 6.0580\n   Val Loss: 6.9469\n   LR: 0.001000\n‚úÖ Saved best model with val loss: 6.9469\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:42<00:00,  2.07it/s, loss=6.0492, teacher=0.54]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.36it/s, loss=6.9369]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 6/20\n   Train Loss: 5.9746\n   Val Loss: 7.0144\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:42<00:00,  2.08it/s, loss=5.8287, teacher=0.51]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.44it/s, loss=6.9805]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 7/20\n   Train Loss: 5.8998\n   Val Loss: 6.8058\n   LR: 0.001000\n‚úÖ Saved best model with val loss: 6.8058\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:42<00:00,  2.08it/s, loss=5.9107, teacher=0.49]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.49it/s, loss=6.7529]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 8/20\n   Train Loss: 5.8348\n   Val Loss: 6.6938\n   LR: 0.001000\n‚úÖ Saved best model with val loss: 6.6938\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:42<00:00,  2.08it/s, loss=5.8613, teacher=0.46]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.51it/s, loss=6.8484]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 9/20\n   Train Loss: 5.7857\n   Val Loss: 6.6710\n   LR: 0.001000\n‚úÖ Saved best model with val loss: 6.6710\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:41<00:00,  2.10it/s, loss=5.8987, teacher=0.44]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.50it/s, loss=6.7190]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 10/20\n   Train Loss: 5.7403\n   Val Loss: 6.6176\n   LR: 0.001000\n‚úÖ Saved best model with val loss: 6.6176\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:42<00:00,  2.09it/s, loss=5.5091, teacher=0.42]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.49it/s, loss=6.6606]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 11/20\n   Train Loss: 5.7004\n   Val Loss: 6.6184\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:41<00:00,  2.09it/s, loss=5.6428, teacher=0.40]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.48it/s, loss=6.6276]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 12/20\n   Train Loss: 5.6646\n   Val Loss: 6.4917\n   LR: 0.001000\n‚úÖ Saved best model with val loss: 6.4917\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:42<00:00,  2.09it/s, loss=6.0645, teacher=0.38]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.49it/s, loss=6.6706]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 13/20\n   Train Loss: 5.6378\n   Val Loss: 6.5983\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:41<00:00,  2.09it/s, loss=5.8301, teacher=0.36]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.46it/s, loss=6.6591]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 14/20\n   Train Loss: 5.6247\n   Val Loss: 6.5397\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:41<00:00,  2.10it/s, loss=5.8361, teacher=0.34]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.49it/s, loss=6.5196]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 15/20\n   Train Loss: 5.6048\n   Val Loss: 6.4933\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:41<00:00,  2.09it/s, loss=5.4131, teacher=0.32]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.48it/s, loss=6.5524]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 16/20\n   Train Loss: 5.5387\n   Val Loss: 6.4384\n   LR: 0.000500\n‚úÖ Saved best model with val loss: 6.4384\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:41<00:00,  2.10it/s, loss=5.8234, teacher=0.31]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.49it/s, loss=6.5286]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 17/20\n   Train Loss: 5.5414\n   Val Loss: 6.4728\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:41<00:00,  2.09it/s, loss=5.5788, teacher=0.29]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.46it/s, loss=6.5509]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 18/20\n   Train Loss: 5.5351\n   Val Loss: 6.3695\n   LR: 0.000500\n‚úÖ Saved best model with val loss: 6.3695\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:43<00:00,  2.06it/s, loss=5.2958, teacher=0.28]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.48it/s, loss=6.4334]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 19/20\n   Train Loss: 5.5170\n   Val Loss: 6.3651\n   LR: 0.000500\n‚úÖ Saved best model with val loss: 6.3651\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:42<00:00,  2.09it/s, loss=5.6870, teacher=0.26]\n[Validate]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.41it/s, loss=6.5123]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Epoch 20/20\n   Train Loss: 5.5107\n   Val Loss: 6.3517\n   LR: 0.000500\n‚úÖ Saved best model with val loss: 6.3517\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"print(\"\\nFinal Evaluation\")\nprint(\"=\" * 50)\n\n# Load best model\ncheckpoint = torch.load(\"best_model_attention.pt\", weights_only=False)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\nprint(f\"Model loaded from best_model_attention.pt\")\nprint(f\"Epoch: {checkpoint.get('epoch', 'N/A')}\")\nprint(f\"Loss: {checkpoint.get('loss', 'N/A'):.4f}\" if 'loss' in checkpoint else \"\")\n\n# Calculate metrics\nprint(\"\\nCalculating BLEU score with greedy decoding...\")\nbleu_score_greedy = calculate_bleu(model, test_data, n_samples=200, use_beam_search=False)\nprint(f\"\\n{'='*50}\")\nprint(f\"MEAN BLEU SCORE (Greedy): {bleu_score_greedy:.4f}\")\nprint(f\"{'='*50}\")\n\n# Optionally calculate with beam search as well\nprint(\"\\nCalculating BLEU score with beam search...\")\nbleu_score_beam = calculate_bleu(model, test_data, n_samples=100, use_beam_search=True)\nprint(f\"\\n{'='*50}\")\nprint(f\"MEAN BLEU SCORE (Beam Search): {bleu_score_beam:.4f}\")\nprint(f\"{'='*50}\")\n\n# Summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL RESULTS SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Greedy Decoding BLEU: {bleu_score_greedy:.4f}\")\nprint(f\"Beam Search BLEU:     {bleu_score_beam:.4f}\")\nprint(f\"Best Method: {'Beam Search' if bleu_score_beam > bleu_score_greedy else 'Greedy'}\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T10:55:33.513877Z","iopub.execute_input":"2026-02-15T10:55:33.514517Z","iopub.status.idle":"2026-02-15T10:57:02.323391Z","shell.execute_reply.started":"2026-02-15T10:55:33.514492Z","shell.execute_reply":"2026-02-15T10:57:02.322721Z"}},"outputs":[{"name":"stdout","text":"\nFinal Evaluation\n==================================================\nModel loaded from best_model_attention.pt\nEpoch: 19\n\n\nCalculating BLEU score with greedy decoding...\n","output_type":"stream"},{"name":"stderr","text":"Calculating BLEU: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:20<00:00,  9.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nMEAN BLEU SCORE (Greedy): 0.0107\n==================================================\n\nCalculating BLEU score with beam search...\n","output_type":"stream"},{"name":"stderr","text":"Calculating BLEU: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:08<00:00,  1.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nMEAN BLEU SCORE (Beam Search): 0.0131\n==================================================\n\n==================================================\nFINAL RESULTS SUMMARY\n==================================================\nGreedy Decoding BLEU: 0.0107\nBeam Search BLEU:     0.0131\nBest Method: Beam Search\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":58}]}