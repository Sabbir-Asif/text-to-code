{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4dc651d",
   "metadata": {},
   "source": [
    "# Assignment: Text-to-Python Code Generation Using Seq2Seq RNN Models\n",
    "\n",
    "## Objective\n",
    "Implement and compare three RNN architectures for code generation:\n",
    "1. **Vanilla RNN Seq2Seq** - Baseline with fixed-length context\n",
    "2. **LSTM Seq2Seq** - Improved long-term dependency handling\n",
    "3. **LSTM with Attention** - Remove context bottleneck\n",
    "\n",
    "## Dataset\n",
    "- **Source**: CodeSearchNet Python (Hugging Face)\n",
    "- **Input**: Natural language docstrings (max 50 tokens)\n",
    "- **Output**: Python function code (max 80 tokens)\n",
    "- **Split**: 10,000 training | 1,500 validation | 1,500 test\n",
    "\n",
    "## Configuration\n",
    "- Embedding Dimension: 256\n",
    "- Hidden Dimension: 256\n",
    "- Batch Size: 64\n",
    "- Epochs: 20\n",
    "- Learning Rate: 0.001\n",
    "- Teacher Forcing Ratio: 0.5\n",
    "- Loss: Cross-Entropy with padding ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers torch torchvision torchaudio nltk sacrebleu matplotlib pandas numpy tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a51bb3",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Configuration\n",
    "CONFIG = {\n",
    "    # Dataset\n",
    "    'TRAIN_SIZE': 10000,\n",
    "    'VAL_SIZE': 1500,\n",
    "    'TEST_SIZE': 1500,\n",
    "    \n",
    "    # Sequence lengths\n",
    "    'MAX_DOCSTRING_LEN': 50,\n",
    "    'MAX_CODE_LEN': 80,\n",
    "    \n",
    "    # Architecture\n",
    "    'EMBEDDING_DIM': 256,\n",
    "    'HIDDEN_DIM': 256,\n",
    "    'NUM_LAYERS': 2,\n",
    "    'DROPOUT': 0.3,\n",
    "    'BIDIRECTIONAL': True,\n",
    "    \n",
    "    # Training\n",
    "    'BATCH_SIZE': 64,\n",
    "    'EPOCHS': 20,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'TEACHER_FORCING_RATIO': 0.5,\n",
    "    'VOCAB_SIZE': 5000,\n",
    "    'GRADIENT_CLIP': 1.0,\n",
    "    \n",
    "    # Advanced optimization\n",
    "    'WARMUP_STEPS': 1000,\n",
    "    'EARLY_STOPPING_PATIENCE': 3,\n",
    "    'BEAM_SIZE': 3,\n",
    "    'SCHEDULED_SAMPLING': True,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "# Save config to JSON for reproducibility\n",
    "import json\n",
    "with open('models/config.json', 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(\"\\n✓ Configuration saved to models/config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2370f",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare CodeSearchNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading CodeSearchNet Python dataset...\")\n",
    "dataset = load_dataset(\"Nan-Do/code-search-net-python\", split='train')\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "\n",
    "# Filter dataset to only include items with both docstring and code\n",
    "print(\"Filtering dataset...\")\n",
    "dataset = dataset.filter(\n",
    "    lambda x: x['docstring'] is not None \n",
    "    and x['code'] is not None \n",
    "    and len(x['docstring'].strip()) > 0 \n",
    "    and len(x['code'].strip()) > 0\n",
    ")\n",
    "print(f\"Filtered dataset size: {len(dataset)}\")\n",
    "\n",
    "# Calculate total needed\n",
    "total_needed = CONFIG['TRAIN_SIZE'] + CONFIG['VAL_SIZE'] + CONFIG['TEST_SIZE']\n",
    "print(f\"Total samples needed: {total_needed}\")\n",
    "\n",
    "# Sample and shuffle\n",
    "dataset = dataset.shuffle(seed=SEED).select(range(min(total_needed, len(dataset))))\n",
    "print(f\"Sampled {len(dataset)} examples\")\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "docstring_lengths = [len(x['docstring'].split()) for x in dataset]\n",
    "code_lengths = [len(x['code'].split()) for x in dataset]\n",
    "print(f\"Docstring length - Mean: {np.mean(docstring_lengths):.1f}, Max: {np.max(docstring_lengths)}\")\n",
    "print(f\"Code length - Mean: {np.mean(code_lengths):.1f}, Max: {np.max(code_lengths)}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample example:\")\n",
    "sample = dataset[0]\n",
    "print(f\"Docstring: {sample['docstring'][:100]}...\")\n",
    "print(f\"Code:\\n{sample['code'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f64455",
   "metadata": {},
   "source": [
    "## 3. Build Tokenizer and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7915b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"Simple whitespace-based tokenizer with special tokens\"\"\"\n",
    "    def __init__(self, vocab_size=5000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.vocab_built = False\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Simple whitespace tokenization with special character handling\"\"\"\n",
    "        text = text.lower()\n",
    "        # Add spaces around special characters\n",
    "        text = re.sub(r'([\\(\\)\\[\\]\\{\\}:,\\.=\\+\\-\\*\\/])', r' \\1 ', text)\n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        print(f\"Building vocabulary from {len(texts)} texts...\")\n",
    "        counter = Counter()\n",
    "        for text in tqdm(texts, desc=\"Building vocab\"):\n",
    "            tokens = self.tokenize(text)\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        # Get most common tokens\n",
    "        most_common = counter.most_common(self.vocab_size - 4)\n",
    "        for idx, (word, _) in enumerate(most_common, start=4):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "        \n",
    "        self.vocab_built = True\n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "    \n",
    "    def encode(self, text, max_len=None, add_special_tokens=True):\n",
    "        \"\"\"Encode text to token indices\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        if max_len and len(tokens) > max_len - 2:\n",
    "            tokens = tokens[:max_len - 2]\n",
    "        \n",
    "        indices = [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            indices = [self.word2idx['<SOS>']] + indices + [self.word2idx['<EOS>']]\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices, skip_special_tokens=True):\n",
    "        \"\"\"Decode token indices to text\"\"\"\n",
    "        tokens = []\n",
    "        for idx in indices:\n",
    "            if idx in self.idx2word:\n",
    "                token = self.idx2word[idx]\n",
    "                if skip_special_tokens and token in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:\n",
    "                    if token == '<EOS>':\n",
    "                        break\n",
    "                    continue\n",
    "                tokens.append(token)\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save tokenizer to file\"\"\"\n",
    "        data = {\n",
    "            'word2idx': self.word2idx,\n",
    "            'idx2word': {str(k): v for k, v in self.idx2word.items()},\n",
    "            'vocab_size': self.vocab_size\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        print(f\"Tokenizer saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"Load tokenizer from file\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        tokenizer = cls(vocab_size=data['vocab_size'])\n",
    "        tokenizer.word2idx = data['word2idx']\n",
    "        tokenizer.idx2word = {int(k): v for k, v in data['idx2word'].items()}\n",
    "        tokenizer.vocab_built = True\n",
    "        print(f\"Tokenizer loaded from {filepath}\")\n",
    "        return tokenizer\n",
    "\n",
    "# Build tokenizers\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Building Tokenizers\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "src_tokenizer = Tokenizer(vocab_size=CONFIG['VOCAB_SIZE'])\n",
    "tgt_tokenizer = Tokenizer(vocab_size=CONFIG['VOCAB_SIZE'])\n",
    "\n",
    "# Extract texts\n",
    "docstrings = [item['docstring'] for item in dataset if item['docstring']]\n",
    "codes = [item['code'] for item in dataset if item['code']]\n",
    "\n",
    "# Build vocabularies\n",
    "src_tokenizer.build_vocab(docstrings)\n",
    "tgt_tokenizer.build_vocab(codes)\n",
    "\n",
    "# Save tokenizers for later use\n",
    "os.makedirs('models', exist_ok=True)\n",
    "src_tokenizer.save('models/src_tokenizer.json')\n",
    "tgt_tokenizer.save('models/tgt_tokenizer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d28030e",
   "metadata": {},
   "source": [
    "## 4. Create Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7150ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeDataset(Dataset):\n",
    "    \"\"\"Dataset for code generation from docstrings\"\"\"\n",
    "    def __init__(self, data, src_tokenizer, tgt_tokenizer, max_src_len, max_tgt_len):\n",
    "        self.data = list(data) if not isinstance(data, list) else data\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        src = self.src_tokenizer.encode(item['docstring'], max_len=self.max_src_len)\n",
    "        tgt = self.tgt_tokenizer.encode(item['code'], max_len=self.max_tgt_len)\n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for padding sequences\"\"\"\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# Split dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Creating Train/Val/Test Splits\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_data = dataset.select(range(CONFIG['TRAIN_SIZE']))\n",
    "val_data = dataset.select(range(CONFIG['TRAIN_SIZE'], CONFIG['TRAIN_SIZE'] + CONFIG['VAL_SIZE']))\n",
    "test_data = dataset.select(range(\n",
    "    CONFIG['TRAIN_SIZE'] + CONFIG['VAL_SIZE'],\n",
    "    CONFIG['TRAIN_SIZE'] + CONFIG['VAL_SIZE'] + CONFIG['TEST_SIZE']\n",
    "))\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CodeDataset(train_data, src_tokenizer, tgt_tokenizer,\n",
    "                           CONFIG['MAX_DOCSTRING_LEN'], CONFIG['MAX_CODE_LEN'])\n",
    "val_dataset = CodeDataset(val_data, src_tokenizer, tgt_tokenizer,\n",
    "                         CONFIG['MAX_DOCSTRING_LEN'], CONFIG['MAX_CODE_LEN'])\n",
    "test_dataset = CodeDataset(test_data, src_tokenizer, tgt_tokenizer,\n",
    "                          CONFIG['MAX_DOCSTRING_LEN'], CONFIG['MAX_CODE_LEN'])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'],\n",
    "                         shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'],\n",
    "                       shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['BATCH_SIZE'],\n",
    "                        shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"\\nDataloaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825e7dd9",
   "metadata": {},
   "source": [
    "## 5. Model 1: Vanilla RNN Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNNEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, bidirectional=True):\n",
    "        super(VanillaRNNEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "class VanillaRNNDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        super(VanillaRNNDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden\n",
    "\n",
    "class VanillaRNNSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(VanillaRNNSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode\n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        # For bidirectional encoder, need to process hidden state carefully\n",
    "        if self.encoder.bidirectional:\n",
    "            # Reshape from (num_directions, batch, hidden_dim) to (batch, num_directions*hidden_dim)\n",
    "            num_directions = hidden.shape[0] // (batch_size) if hidden.shape[0] > batch_size else 2\n",
    "            # More explicit: take both directions and concatenate\n",
    "            hidden_fwd = hidden[0].unsqueeze(0)  # (1, batch, hidden_dim)\n",
    "            hidden_bwd = hidden[1].unsqueeze(0)  # (1, batch, hidden_dim)\n",
    "            # Average them for single-direction decoder input\n",
    "            hidden = ((hidden_fwd + hidden_bwd) / 2).contiguous()\n",
    "        \n",
    "        # Decode\n",
    "        decoder_input = tgt[:, 0].unsqueeze(1)\n",
    "        for t in range(1, tgt_len):\n",
    "            prediction, hidden = self.decoder(decoder_input, hidden)\n",
    "            outputs[:, t, :] = prediction\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = prediction.argmax(1)\n",
    "            decoder_input = tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"✓ VanillaRNNSeq2Seq model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b271e",
   "metadata": {},
   "source": [
    "## 6. Model 2: LSTM Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f9148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.3, bidirectional=True):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                           bidirectional=bidirectional)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode\n",
    "        _, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Handle bidirectional encoder output (if needed)\n",
    "        if self.encoder.bidirectional:\n",
    "            # For bidirectional, hidden/cell shape: (num_layers*2, batch, hidden_dim)\n",
    "            # Reshape to (num_layers, batch, 2*hidden_dim) then average directions\n",
    "            num_layers = self.encoder.num_layers\n",
    "            batch_size = hidden.shape[1]\n",
    "            hidden_dim = hidden.shape[2]\n",
    "            \n",
    "            # Reshape and average forward/backward for each layer\n",
    "            hidden_reshaped = hidden.view(num_layers, 2, batch_size, hidden_dim)\n",
    "            cell_reshaped = cell.view(num_layers, 2, batch_size, hidden_dim)\n",
    "            \n",
    "            hidden = (hidden_reshaped[:, 0] + hidden_reshaped[:, 1]).contiguous() / 2\n",
    "            cell = (cell_reshaped[:, 0] + cell_reshaped[:, 1]).contiguous() / 2\n",
    "        \n",
    "        # Decode\n",
    "        decoder_input = tgt[:, 0].unsqueeze(1)\n",
    "        for t in range(1, tgt_len):\n",
    "            prediction, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            outputs[:, t, :] = prediction\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = prediction.argmax(1)\n",
    "            decoder_input = tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"✓ LSTMSeq2Seq model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc58aaaf",
   "metadata": {},
   "source": [
    "## 7. Model 3: LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Bahdanau (Additive) Attention Mechanism\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        # Both hidden and context are projected to hidden_dim, so attention input is 2*hidden_dim\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        hidden: (1, batch, hidden_dim)\n",
    "        encoder_outputs: (batch, src_len, hidden_dim) - already projected\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat hidden state for each source token\n",
    "        hidden_expanded = hidden.squeeze(0).unsqueeze(1).repeat(1, src_len, 1)  # (batch, src_len, hidden_dim)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden_expanded, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, encoder_output_dim=None):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        if encoder_output_dim is None:\n",
    "            encoder_output_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Project encoder outputs to hidden_dim if they're larger (bidirectional case)\n",
    "        self.encoder_output_dim = encoder_output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        if encoder_output_dim != hidden_dim:\n",
    "            self.encoder_proj = nn.Linear(encoder_output_dim, hidden_dim)\n",
    "        else:\n",
    "            self.encoder_proj = None\n",
    "        \n",
    "        # Attention now works with hidden_dim only (since encoder outputs are projected)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Project encoder outputs if needed\n",
    "        if self.encoder_proj is not None:\n",
    "            encoder_outputs_proj = self.encoder_proj(encoder_outputs)\n",
    "        else:\n",
    "            encoder_outputs_proj = encoder_outputs\n",
    "        \n",
    "        # Calculate attention weights\n",
    "        attn_weights = self.attention(hidden, encoder_outputs_proj)\n",
    "        attn_weights_expanded = attn_weights.unsqueeze(1)\n",
    "        \n",
    "        # Apply attention to projected encoder outputs\n",
    "        context = torch.bmm(attn_weights_expanded, encoder_outputs_proj)\n",
    "        \n",
    "        # Concatenate embedding and context\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        \n",
    "        return prediction, hidden, cell, attn_weights\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM Encoder\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.hidden_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # Combine bidirectional hidden states: shape (2, batch, hidden_dim) -> (1, batch, hidden_dim)\n",
    "        # Average the forward and backward directions\n",
    "        hidden_fwd = hidden[0].unsqueeze(0)  # (1, batch, hidden_dim)\n",
    "        hidden_bwd = hidden[1].unsqueeze(0)  # (1, batch, hidden_dim)\n",
    "        hidden = ((hidden_fwd + hidden_bwd) / 2).contiguous()\n",
    "        \n",
    "        cell_fwd = cell[0].unsqueeze(0)      # (1, batch, hidden_dim)\n",
    "        cell_bwd = cell[1].unsqueeze(0)      # (1, batch, hidden_dim)\n",
    "        cell = ((cell_fwd + cell_bwd) / 2).contiguous()\n",
    "        \n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class LSTMAttentionSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(LSTMAttentionSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        attentions = torch.zeros(batch_size, tgt_len, src.shape[1]).to(self.device)\n",
    "        \n",
    "        # Encode\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Handle bidirectional encoder output\n",
    "        if self.encoder.bidirectional:\n",
    "            # For bidirectional, hidden/cell shape: (num_layers*2, batch, hidden_dim)\n",
    "            # Need to reshape for single-layer decoder\n",
    "            num_layers = self.encoder.num_layers\n",
    "            hidden_dim = hidden.shape[2]\n",
    "            \n",
    "            # Reshape and average forward/backward for last layer only (for decoder input)\n",
    "            hidden_reshaped = hidden.view(num_layers, 2, batch_size, hidden_dim)\n",
    "            cell_reshaped = cell.view(num_layers, 2, batch_size, hidden_dim)\n",
    "            \n",
    "            # Take only the last layer and average forward/backward\n",
    "            hidden = ((hidden_reshaped[-1, 0] + hidden_reshaped[-1, 1]) / 2).unsqueeze(0).contiguous()\n",
    "            cell = ((cell_reshaped[-1, 0] + cell_reshaped[-1, 1]) / 2).unsqueeze(0).contiguous()\n",
    "        \n",
    "        # Decode\n",
    "        decoder_input = tgt[:, 0].unsqueeze(1)\n",
    "        for t in range(1, tgt_len):\n",
    "            prediction, hidden, cell, attn_weights = self.decoder(decoder_input, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t, :] = prediction\n",
    "            attentions[:, t, :] = attn_weights\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = prediction.argmax(1)\n",
    "            decoder_input = tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "        \n",
    "        return outputs, attentions\n",
    "\n",
    "print(\"✓ LSTMAttentionSeq2Seq model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdf245",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b526ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device, teacher_forcing_ratio, use_attention=False):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, tgt in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_attention:\n",
    "            output, _ = model(src, tgt, teacher_forcing_ratio)\n",
    "        else:\n",
    "            output = model(src, tgt, teacher_forcing_ratio)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['GRADIENT_CLIP'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, use_attention=False):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            if use_attention:\n",
    "                output, _ = model(src, tgt, teacher_forcing_ratio=0)\n",
    "            else:\n",
    "                output = model(src, tgt, teacher_forcing_ratio=0)\n",
    "            \n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device,\n",
    "                epochs, teacher_forcing_ratio, model_name, use_attention=False):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device,\n",
    "                                teacher_forcing_ratio, use_attention)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device, use_attention)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\", end=\"\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'loss': val_loss,\n",
    "                'config': CONFIG\n",
    "            }, f'models/{model_name}_best.pt')\n",
    "            print(\" ✓ (saved)\")\n",
    "        else:\n",
    "            print()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"Best model: {model_name}_best.pt (Epoch {best_epoch+1}, Loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282488ca",
   "metadata": {},
   "source": [
    "## 9. Train Model 1: Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d38497",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 1: VANILLA RNN SEQ2SEQ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize model\n",
    "src_vocab_size = len(src_tokenizer.word2idx)\n",
    "tgt_vocab_size = len(tgt_tokenizer.word2idx)\n",
    "\n",
    "rnn_encoder = VanillaRNNEncoder(src_vocab_size, CONFIG['EMBEDDING_DIM'], CONFIG['HIDDEN_DIM'],\n",
    "                               dropout=CONFIG['DROPOUT'], bidirectional=CONFIG['BIDIRECTIONAL'])\n",
    "rnn_decoder = VanillaRNNDecoder(tgt_vocab_size, CONFIG['EMBEDDING_DIM'], CONFIG['HIDDEN_DIM'],\n",
    "                               dropout=CONFIG['DROPOUT'])\n",
    "rnn_model = VanillaRNNSeq2Seq(rnn_encoder, rnn_decoder, device).to(device)\n",
    "\n",
    "# Count parameters\n",
    "rnn_params = sum(p.numel() for p in rnn_model.parameters() if p.requires_grad)\n",
    "print(f\"Parameters: {rnn_params:,}\\n\")\n",
    "\n",
    "# Setup training\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Train\n",
    "rnn_train_losses, rnn_val_losses = train_model(\n",
    "    rnn_model, train_loader, val_loader, rnn_optimizer, criterion, device,\n",
    "    CONFIG['EPOCHS'], CONFIG['TEACHER_FORCING_RATIO'], 'vanilla_rnn', use_attention=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f5c202",
   "metadata": {},
   "source": [
    "## 10. Train Model 2: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 2: LSTM SEQ2SEQ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize model\n",
    "lstm_encoder = LSTMEncoder(src_vocab_size, CONFIG['EMBEDDING_DIM'], CONFIG['HIDDEN_DIM'],\n",
    "                          num_layers=CONFIG['NUM_LAYERS'], dropout=CONFIG['DROPOUT'],\n",
    "                          bidirectional=CONFIG['BIDIRECTIONAL'])\n",
    "lstm_decoder = LSTMDecoder(tgt_vocab_size, CONFIG['EMBEDDING_DIM'], CONFIG['HIDDEN_DIM'],\n",
    "                          num_layers=CONFIG['NUM_LAYERS'], dropout=CONFIG['DROPOUT'])\n",
    "lstm_model = LSTMSeq2Seq(lstm_encoder, lstm_decoder, device).to(device)\n",
    "\n",
    "# Count parameters\n",
    "lstm_params = sum(p.numel() for p in lstm_model.parameters() if p.requires_grad)\n",
    "print(f\"Parameters: {lstm_params:,}\\n\")\n",
    "\n",
    "# Setup training\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
    "\n",
    "# Train\n",
    "lstm_train_losses, lstm_val_losses = train_model(lstm_model, train_loader, val_loader, lstm_optimizer, criterion, device, CONFIG['EPOCHS'], CONFIG['TEACHER_FORCING_RATIO'], 'lstm', use_attention=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09acfff",
   "metadata": {},
   "source": [
    "## 11. Train Model 3: LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 3: LSTM WITH ATTENTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize model with bidirectional encoder\n",
    "attn_encoder = LSTMEncoder(src_vocab_size, CONFIG['EMBEDDING_DIM'], CONFIG['HIDDEN_DIM'],\n",
    "                          num_layers=CONFIG['NUM_LAYERS'], dropout=CONFIG['DROPOUT'],\n",
    "                          bidirectional=CONFIG['BIDIRECTIONAL'])\n",
    "# Encoder is bidirectional, so output dim is HIDDEN_DIM * 2\n",
    "encoder_output_dim = CONFIG['HIDDEN_DIM'] * 2 if CONFIG['BIDIRECTIONAL'] else CONFIG['HIDDEN_DIM']\n",
    "attn_decoder = AttentionDecoder(tgt_vocab_size, CONFIG['EMBEDDING_DIM'], CONFIG['HIDDEN_DIM'],\n",
    "                               encoder_output_dim=encoder_output_dim)\n",
    "attn_model = LSTMAttentionSeq2Seq(attn_encoder, attn_decoder, device).to(device)\n",
    "\n",
    "# Count parameters\n",
    "attn_params = sum(p.numel() for p in attn_model.parameters() if p.requires_grad)\n",
    "print(f\"Parameters: {attn_params:,}\\n\")\n",
    "\n",
    "# Setup training\n",
    "attn_optimizer = optim.Adam(attn_model.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
    "\n",
    "# Train\n",
    "attn_train_losses, attn_val_losses = train_model(attn_model, train_loader, val_loader, attn_optimizer, criterion, device, CONFIG['EPOCHS'], CONFIG['TEACHER_FORCING_RATIO'], 'lstm_attention', use_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc3c9c",
   "metadata": {},
   "source": [
    "## 12. Save Training Results and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3da531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "import pickle\n",
    "\n",
    "training_history = {\n",
    "    'vanilla_rnn': {'train_losses': rnn_train_losses, 'val_losses': rnn_val_losses},\n",
    "    'lstm': {'train_losses': lstm_train_losses, 'val_losses': lstm_val_losses},\n",
    "    'lstm_attention': {'train_losses': attn_train_losses, 'val_losses': attn_val_losses}\n",
    "}\n",
    "\n",
    "with open('models/training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(training_history, f)\n",
    "\n",
    "# Save model parameters\n",
    "model_params = {\n",
    "    'vanilla_rnn': rnn_params,\n",
    "    'lstm': lstm_params,\n",
    "    'lstm_attention': attn_params\n",
    "}\n",
    "\n",
    "with open('models/model_params.json', 'w') as f:\n",
    "    json.dump(model_params, f, indent=2)\n",
    "\n",
    "# Save configuration\n",
    "with open('models/config.json', 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSaved files in 'models/' directory:\")\n",
    "print(\"  ✓ vanilla_rnn_best.pt\")\n",
    "print(\"  ✓ lstm_best.pt\")\n",
    "print(\"  ✓ lstm_attention_best.pt\")\n",
    "print(\"  ✓ training_history.pkl\")\n",
    "print(\"  ✓ model_params.json\")\n",
    "print(\"  ✓ config.json\")\n",
    "print(\"  ✓ src_tokenizer.json\")\n",
    "print(\"  ✓ tgt_tokenizer.json\")\n",
    "print(\"\\nReady for analytics on MacBook M1!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
